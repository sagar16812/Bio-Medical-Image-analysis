<!DOCTYPE html>
<!-- saved from url=(0112)https://medium.com/tensorflow/an-introduction-to-biomedical-image-analysis-with-tensorflow-and-dltk-2c25304e7c13 -->
<html lang="en" class="gr__medium_com" data-rh="lang"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script async="" src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/branch-latest.min.js"></script><script async="" src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/analytics.js"></script><script>!function(c,f){var t,o,i,e=[],r={passive:!0,capture:!0},n=new Date,a="pointerup",u="pointercancel";function p(n,e){t||(t=e,o=n,i=new Date,w(f),s())}function s(){0<=o&&o<i-n&&(e.forEach(function(n){n(o,t)}),e=[])}function l(n){if(n.cancelable){var e=(1e12<n.timeStamp?new Date:performance.now())-n.timeStamp;"pointerdown"==n.type?function(n,e){function t(){p(n,e),i()}function o(){i()}function i(){f(a,t,r),f(u,o,r)}c(a,t,r),c(u,o,r)}(e,n):p(e,n)}}function w(e){["click","mousedown","keydown","touchstart","pointerdown"].forEach(function(n){e(n,l,r)})}w(c),self.perfMetrics=self.perfMetrics||{},self.perfMetrics.onFirstInputDelay=function(n){e.push(n),s()}}(addEventListener,removeEventListener)</script><title>An Introduction to Biomedical Image Analysis with TensorFlow and DLTK</title><meta data-rh="true" name="theme-color" content="#000000"><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"><meta data-rh="true" property="al:ios:app_name" content="Medium"><meta data-rh="true" property="al:ios:app_store_id" content="828256236"><meta data-rh="true" property="al:android:package" content="com.medium.reader"><meta data-rh="true" property="fb:app_id" content="542599432471018"><meta data-rh="true" property="og:site_name" content="Medium"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2018-07-03T22:57:03.262Z"><meta data-rh="true" name="title" content="An Introduction to Biomedical Image Analysis with TensorFlow and DLTK"><meta data-rh="true" property="og:title" content="An Introduction to Biomedical Image Analysis with TensorFlow and DLTK"><meta data-rh="true" property="twitter:title" content="An Introduction to Biomedical Image Analysis with TensorFlow and DLTK"><meta data-rh="true" name="twitter:site" content="@tensorflow"><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/2c25304e7c13"><meta data-rh="true" property="al:android:url" content="medium://p/2c25304e7c13"><meta data-rh="true" property="al:ios:url" content="medium://p/2c25304e7c13"><meta data-rh="true" name="apple-itunes-app" content="app-id=828256236,app-argument=medium://p/2c25304e7c13"><meta data-rh="true" property="al:android:app_name" content="Medium"><meta data-rh="true" name="description" content="DLTK, the Deep Learning Toolkit for Medical Imaging extends TensorFlow to enable deep learning on biomedical images. It provides specialty ops and functions, implementations of models, tutorials (as…"><meta data-rh="true" property="og:description" content="By Martin Rajchl, S. Ira Ktena and Nick Pawlowski — Imperial College London"><meta data-rh="true" property="twitter:description" content="By Martin Rajchl, S. Ira Ktena and Nick Pawlowski — Imperial College London"><meta data-rh="true" property="og:url" content="https://medium.com/tensorflow/an-introduction-to-biomedical-image-analysis-with-tensorflow-and-dltk-2c25304e7c13"><meta data-rh="true" property="al:web:url" content="https://medium.com/tensorflow/an-introduction-to-biomedical-image-analysis-with-tensorflow-and-dltk-2c25304e7c13"><meta data-rh="true" property="og:image" content="https://miro.medium.com/max/1200/1*OjngIU6_yw_cIT-b8ConDA.png"><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/max/1200/1*OjngIU6_yw_cIT-b8ConDA.png"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="article:author" content="/@m_rajchl"><meta data-rh="true" name="twitter:creator" content="@m_rajchl"><meta data-rh="true" name="author" content="Martin Rajchl"><meta data-rh="true" name="robots" content="index,follow"><meta data-rh="true" name="referrer" content="unsafe-url"><meta data-rh="true" name="twitter:label1" value="Reading time"><meta data-rh="true" name="twitter:data1" value="12 min read"><link data-rh="true" rel="publisher" href="https://plus.google.com/103654360130207659246"><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="https://medium.com/osd.xml"><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://cdn-images-1.medium.com/fit/c/120/120/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://cdn-images-1.medium.com/fit/c/76/76/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://cdn-images-1.medium.com/fit/c/60/60/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg" color="#171717"><link data-rh="true" rel="icon" href="https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium.3Y6xpZ-0FSdWDnPM3hSBIA.ico"><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/m2.css"><link data-rh="true" rel="author" href="https://medium.com/@m_rajchl"><link data-rh="true" rel="canonical" href="https://medium.com/tensorflow/an-introduction-to-biomedical-image-analysis-with-tensorflow-and-dltk-2c25304e7c13"><script data-rh="true">(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-24232453-2', 'auto');
ga('send', 'pageview');</script><style type="text/css" data-fela-rehydration="361" data-fela-type="STATIC">html{box-sizing:border-box}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}</style><style type="text/css" data-fela-rehydration="361" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-moz-keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-webkit-keyframes k2{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@-moz-keyframes k2{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@keyframes k2{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@-webkit-keyframes k3{0%{transform:matrix(0.97, 0, 0, 1, 0, 12);opacity:0}20%{transform:matrix(0.99, 0, 0, 1, 0, 2);opacity:0.7}40%{transform:matrix(1, 0, 0, 1, 0, -1);opacity:1}70%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}100%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}}@-moz-keyframes k3{0%{transform:matrix(0.97, 0, 0, 1, 0, 12);opacity:0}20%{transform:matrix(0.99, 0, 0, 1, 0, 2);opacity:0.7}40%{transform:matrix(1, 0, 0, 1, 0, -1);opacity:1}70%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}100%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}}@keyframes k3{0%{transform:matrix(0.97, 0, 0, 1, 0, 12);opacity:0}20%{transform:matrix(0.99, 0, 0, 1, 0, 2);opacity:0.7}40%{transform:matrix(1, 0, 0, 1, 0, -1);opacity:1}70%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}100%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}}</style><style type="text/css" data-fela-rehydration="361" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.n{display:block}.o{position:fixed}.p{top:0}.q{left:0}.r{right:0}.s{z-index:500}.t{box-shadow:0 4px 12px 0 rgba(0, 0, 0, 0.05)}.u{transition:transform 300ms ease}.v{will-change:transform}.w{padding-left:24px}.x{padding-right:24px}.y{margin-left:auto}.z{margin-right:auto}.ab{height:65px}.ac{width:100%}.ae{max-width:1080px}.af{box-sizing:border-box}.ag{display:flex}.ah{align-items:center}.ak{flex:1 0 auto}.al{margin-left:-6px}.am{fill:rgba(0, 0, 0, 0.84)}.an{flex:0 0 auto}.ao{font-family:medium-content-sans-serif-font, "Lucida Grande", "Lucida Sans Unicode", "Lucida Sans", Geneva, Arial, sans-serif}.ap{font-style:normal}.aq{line-height:20px}.ar{font-size:15.8px}.as{letter-spacing:0px}.at{color:rgba(0, 0, 0, 0.54)}.au{fill:rgba(0, 0, 0, 0.54)}.av{color:inherit}.aw{fill:inherit}.ax{font-size:inherit}.ay{border:inherit}.az{font-family:inherit}.ba{letter-spacing:inherit}.bb{font-weight:inherit}.bc{padding:0}.bd{margin:0}.be:hover{cursor:pointer}.bf:hover{color:rgba(0, 0, 0, 0.9)}.bg:hover{fill:rgba(0, 0, 0, 0.9)}.bh:focus{outline:none}.bi:disabled{cursor:default}.bj:disabled{color:rgba(0, 0, 0, 0.54)}.bk:disabled{fill:rgba(0, 0, 0, 0.54)}.bl{margin-left:16px}.bm{margin-right:16px}.bp{padding:4px 12px}.bq{color:rgba(0, 0, 0, 0.84)}.br{background:0}.bs{border-color:rgba(0, 0, 0, 0.54)}.bt:hover{color:rgba(0, 0, 0, 0.97)}.bu:hover{fill:rgba(0, 0, 0, 0.97)}.bv:hover{border-color:rgba(0, 0, 0, 0.84)}.bw{border-radius:4px}.bx{border-width:1px}.by{border-style:solid}.bz{display:inline-block}.ca{text-decoration:none}.cb{padding-bottom:10px}.cc{padding-top:10px}.cd{border-radius:50%}.ce{height:32px}.cf{width:32px}.cg{border-top:1px solid rgba(0, 0, 0, 0.1)}.ci{height:54px}.cj{overflow:hidden}.ck{margin-right:40px}.cl{width:35px}.cm{height:36px}.cn{overflow:auto}.co{flex:0 1 auto}.cp{list-style-type:none}.cq{line-height:40px}.cr{white-space:nowrap}.cs{overflow-x:auto}.ct{align-items:flex-start}.cu{margin-top:20px}.cv{padding-top:20px}.cw{height:80px}.cx{height:20px}.cy{margin-right:15px}.cz{margin-left:15px}.da:first-child{margin-left:0}.db{min-width:1px}.dc{background-color:rgba(0, 0, 0, 0.34)}.dd{font-weight:300}.de{font-size:15px}.df{line-height:21px}.dg{text-transform:uppercase}.dh{letter-spacing:1px}.di{margin-bottom:0px}.dj{height:119px}.dm{padding-bottom:1px}.dn{margin-top:40px}.do{max-width:728px}.dp{opacity:0}.dq{pointer-events:none}.dr{will-change:opacity}.ds{transition:opacity 200ms}.dt{width:131px}.du{left:50%}.dv{transform:translateX(-516px)}.dw{top:calc(65px + 54px + 40px)}.dx{flex-direction:column}.dy{padding-bottom:28px}.dz{border-bottom:1px solid rgba(0, 0, 0, 0.1)}.ea{font-weight:600}.eb{font-size:18px}.ec{padding-bottom:20px}.ed{padding-top:2px}.ee{font-size:16px}.ef{max-height:120px}.eg{text-overflow:ellipsis}.eh{display:-webkit-box}.ei{-webkit-line-clamp:6}.ej{-webkit-box-orient:vertical}.ek{color:rgba(112, 114, 117, 1)}.el{fill:rgba(130, 133, 136, 1)}.em{border-color:rgba(130, 133, 136, 1)}.en:hover{color:rgba(103, 104, 107, 1)}.eo:hover{fill:rgba(112, 114, 117, 1)}.ep:hover{border-color:rgba(112, 114, 117, 1)}.eq{padding-top:28px}.er{margin-bottom:19px}.es{margin-left:-5px}.et{margin-right:5px}.eu{position:relative}.ev{outline:0}.ew{border:0}.ex{user-select:none}.ey{cursor:pointer}.ez> svg{pointer-events:none}.fa:active{border-style:none}.fb{margin-top:5px}.fc button{text-align:left}.fd{fill:rgba(0, 0, 0, 0.76)}.fe{clear:both}.ff{justify-content:center}.fg{padding-top:5px}.fh{margin-right:8px}.fi{margin-bottom:8px}.fj{border-radius:3px}.fk{padding:5px 10px}.fl{background:rgba(0, 0, 0, 0.05)}.fm{line-height:22px}.fn{justify-content:space-between}.fo{margin-top:15px}.fp{border:1px solid rgba(0, 0, 0, 0.1)}.fq{height:60px}.fr{transition:border-color 150ms ease}.fs{width:60px}.ft::before{background:
      radial-gradient(circle, rgba(112, 114, 117, 1) 60%, transparent 70%)
    }.fu::before{border-radius:50%}.fv::before{content:""}.fw::before{display:block}.fx::before{z-index:0}.fy::before{left:0}.fz::before{height:100%}.ga::before{position:absolute}.gb::before{top:0}.gc::before{width:100%}.gd:hover::before{animation:k2 2000ms infinite cubic-bezier(.1,.12,.25,1)}.ge:active{border-style:solid}.gf{background:rgba(255, 255, 255, 1)}.gg{z-index:2}.gh{height:100%}.gi{position:absolute}.gj{padding-right:8px}.gk{display:none}.gl{margin-top:25px}.gm{margin-bottom:25px}.gn{padding-top:32px}.go{border-top:solid 1px rgba(0, 0, 0, 0.1)}.gp{margin-bottom:32px}.gq{min-height:80px}.gv{width:80px}.gw{padding-left:102px}.gy{letter-spacing:0.05em}.gz{margin-bottom:6px}.ha{font-size:28px}.hb{line-height:36px}.hc{max-width:555px}.hd{max-width:450px}.he{line-height:24px}.hg{max-width:550px}.hh{padding-top:25px}.hi{padding:20px}.hj{border:1px solid rgba(130, 133, 136, 1)}.hk{text-align:center}.hl{margin-top:64px}.hm{background-color:rgba(0, 0, 0, 0.02)}.hn{margin:15px 0}.ho{top:calc(100vh + 100px)}.hp{bottom:calc(100vh + 100px)}.hq{width:10px}.hr{word-break:break-word}.hs{word-wrap:break-word}.ht:after{display:block}.hu:after{content:""}.hv:after{clear:both}.hw{margin:0 auto}.hx{line-height:1.23}.hy{letter-spacing:0}.hz{font-family:medium-content-title-font, Georgia, Cambria, "Times New Roman", Times, serif}.ia{font-size:40px}.ig{margin-bottom:-0.27em}.ih{line-height:48px}.ii{margin-top:32px}.ij{height:48px}.ik{width:48px}.il{margin-left:12px}.im{margin-bottom:2px}.io{max-height:20px}.ip{-webkit-line-clamp:1}.iq:hover{text-decoration:underline}.ir{margin-left:8px}.is{padding:0px 8px}.it{line-height:18px}.iu{line-height:1.58}.iv{letter-spacing:-0.004em}.iw{font-family:medium-content-serif-font, Georgia, Cambria, "Times New Roman", Times, serif}.jh{margin-bottom:-0.46em}.ji{background-repeat:repeat-x}.jj{background-image:linear-gradient(to right,rgba(0, 0, 0, 0.84) 100%,rgba(0, 0, 0, 0.84) 0);background-image:url('data:image/svg+xml;utf8,<svg preserveAspectRatio="none" viewBox="0 0 1 1" xmlns="http://www.w3.org/2000/svg"><line x1="0" y1="0" x2="1" y2="1" stroke="rgba(0, 0, 0, 0.84)" /></svg>')}.jk{background-size:1px 1px}.jl{background-position:0 1.05em;background-position:0 calc(1em + 1px)}.jm{font-style:italic}.js{max-width:1024px}.jt{transition:opacity 100ms 400ms}.ju{transform:translateZ(0)}.jv{margin:auto}.jw{background-color:rgba(0, 0, 0, 0.05)}.jx{padding-bottom:50%}.jy{filter:blur(20px)}.jz{transform:scale(1.1)}.ka{line-height:1.4}.kb{margin-top:10px}.kd{line-height:1.12}.ke{letter-spacing:-0.022em}.kp{margin-bottom:-0.28em}.kv{font-weight:700}.kw{max-width:956px}.kx{padding-bottom:53.66108786610879%}.ld{list-style-type:disc}.le{margin-left:30px}.lf{padding-left:0px}.ll{line-height:1.18}.lm{font-family:Menlo, Monaco, "Courier New", Courier, monospace}.ln{margin-top:-0.09em}.lo{margin-bottom:-0.09em}.lp{white-space:pre-wrap}.lq{padding-bottom:NaN%}.lr{max-width:1242px}.ls{padding-bottom:23.993558776167472%}.lt{max-width:1251px}.lu{padding-bottom:63.709032773780976%}.mf{margin-bottom:-0.31em}.mg{max-width:1113px}.mh{padding-bottom:77.62803234501348%}.mi{max-width:1117px}.mj{padding-bottom:58.4601611459266%}.mk{max-width:707px}.ml{padding-bottom:66.47807637906648%}.mm{max-width:1480px}.mn{padding-bottom:30.135135135135133%}.mo{max-width:850px}.mp{padding-bottom:33.882352941176464%}.mq{font-family:medium-content-slab-serif-font, Georgia, Cambria, "Times New Roman", Times, serif}.mr{border:none}.ms{margin-top:30px}.mt:before{content:"..."}.mu:before{letter-spacing:0.6em}.mv:before{text-indent:0.6em}.mw:before{font-style:italic}.mx:before{line-height:1.4}</style><style type="text/css" data-fela-rehydration="361" data-fela-type="RULE" media="screen and (max-width: 1256px)">.d{display:none}</style><style type="text/css" data-fela-rehydration="361" data-fela-type="RULE" media="screen and (max-width: 1080px)">.e{display:none}.kc{text-align:center}</style><style type="text/css" data-fela-rehydration="361" data-fela-type="RULE" media="screen and (max-width: 904px)">.f{display:none}</style><style type="text/css" data-fela-rehydration="361" data-fela-type="RULE" media="screen and (max-width: 728px)">.g{display:none}.ai{height:56px}.aj{display:flex}.bn{margin-left:10px}.bo{margin-right:10px}.ch{display:block}.dk{margin-bottom:0px}.dl{height:110px}.gr{margin-bottom:24px}.gs{align-items:center}.gt{width:102px}.gu{position:relative}.gx{padding-left:0}.hf{margin-top:24px}</style><style type="text/css" data-fela-rehydration="361" data-fela-type="RULE" media="screen and (max-width: 552px)">.h{display:none}.in{margin-bottom:0px}</style><style type="text/css" data-fela-rehydration="361" data-fela-type="RULE" media="screen and (min-width: 1080px)">.i{display:none}.if{margin-top:0.78em}.jf{font-size:21px}.jg{margin-top:2em}.jr{margin-top:56px}.kn{font-size:34px}.ko{margin-top:1.25em}.ku{margin-top:0.86em}.lc{margin-top:1.95em}.lk{margin-top:1.05em}.md{font-size:26px}.me{margin-top:1.72em}</style><style type="text/css" data-fela-rehydration="361" data-fela-type="RULE" media="screen and (min-width: 904px) and (max-width: 1079.98px)">.j{display:none}.ie{margin-top:0.78em}.jd{font-size:21px}.je{margin-top:2em}.jq{margin-top:56px}.kl{font-size:34px}.km{margin-top:1.25em}.kt{margin-top:0.86em}.lb{margin-top:1.95em}.lj{margin-top:1.05em}.mb{font-size:26px}.mc{margin-top:1.72em}</style><style type="text/css" data-fela-rehydration="361" data-fela-type="RULE" media="screen and (min-width: 728px) and (max-width: 903.98px)">.k{display:none}.id{margin-top:0.78em}.jb{font-size:21px}.jc{margin-top:2em}.jp{margin-top:56px}.kj{font-size:34px}.kk{margin-top:1.25em}.ks{margin-top:0.86em}.la{margin-top:1.95em}.li{margin-top:1.05em}.lz{font-size:26px}.ma{margin-top:1.72em}</style><style type="text/css" data-fela-rehydration="361" data-fela-type="RULE" media="screen and (min-width: 552px) and (max-width: 727.98px)">.l{display:none}.ic{margin-top:0.39em}.iz{font-size:18px}.ja{margin-top:1.56em}.jo{margin-top:40px}.kh{font-size:30px}.ki{margin-top:0.93em}.kr{margin-top:0.67em}.kz{margin-top:1.2em}.lh{margin-top:1.34em}.lx{font-size:24px}.ly{margin-top:1.23em}</style><style type="text/css" data-fela-rehydration="361" data-fela-type="RULE" media="screen and (max-width: 551.98px)">.m{display:none}.ib{margin-top:0.39em}.ix{font-size:18px}.iy{margin-top:1.56em}.jn{margin-top:40px}.kf{font-size:30px}.kg{margin-top:0.93em}.kq{margin-top:0.67em}.ky{margin-top:1.2em}.lg{margin-top:1.34em}.lv{font-size:24px}.lw{margin-top:1.23em}</style><script charset="utf-8" src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/vendors_tracing.db265f32.chunk.js"></script><script charset="utf-8" src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/tracing.48bfc3d4.chunk.js"></script><script type="application/ld+json" data-rh="true">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":{"@type":"ImageObject","width":1113,"height":864,"url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F1113\u002F1*OjngIU6_yw_cIT-b8ConDA.png"},"thumbnailUrl":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F1113\u002F1*OjngIU6_yw_cIT-b8ConDA.png","url":"https:\u002F\u002Fmedium.com\u002Ftensorflow\u002Fan-introduction-to-biomedical-image-analysis-with-tensorflow-and-dltk-2c25304e7c13","dateCreated":"2018-07-03T22:57:03.262Z","datePublished":"2018-07-03T22:57:03.262Z","dateModified":"2018-07-03T22:57:03.786Z","headline":"An Introduction to Biomedical Image Analysis with TensorFlow and DLTK","name":"An Introduction to Biomedical Image Analysis with TensorFlow and DLTK","articleId":"2c25304e7c13","keywords":["Lite:true","Tag:Deep Learning","Tag:TensorFlow","Tag:Dltk","Tag:Medical Imaging","Publication:tensorflow","Elevated:false","LockedPostSource:LOCKED_POST_SOURCE_NONE","LayerCake:0"],"author":{"@type":"Person","name":"Martin Rajchl","url":"https:\u002F\u002Fmedium.com\u002F@m_rajchl"},"creator":["Martin Rajchl"],"publisher":{"@type":"Organization","name":"TensorFlow","url":"https:\u002F\u002Fmedium.com\u002Ftensorflow","logo":{"@type":"ImageObject","width":1068,"height":1077,"url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F1068\u002F1*nhv3orDSO8ggJ6q6f-qu4Q.png"}},"mainEntityOfPage":"https:\u002F\u002Fmedium.com\u002Ftensorflow\u002Fan-introduction-to-biomedical-image-analysis-with-tensorflow-and-dltk-2c25304e7c13"}</script><script type="text/javascript" data-rh="true">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src="https://cdn.branch.io/branch-latest.min.js";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,"script","branch",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent".split(" "), 0);
  branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true, 'tracking_disabled': null}, function(err, data) {});</script><meta data-rh="true" name="viewport" content="width=device-width, initial-scale=1"></head><body data-gr-c-s-loaded="true"><div id="root"><div class="a b c"><div class="d e f g h i j k l m hidden-originally"></div><nav class="n o p q r c s t u v" style=""><div class="branch-journeys-top"><div class="n c"><section class="w x y z ab ac ae af ag ah ai aj"><div class="ag ah ak s"><div class="al n"><a href="https://medium.com/" aria-label="Homepage"><svg width="45" height="45" viewBox="0 0 45 45" class="am"><path d="M5 40V5h35v35H5zm8.56-12.63c0 .56-.03.69-.32 1.03L10.8 31.4v.4h6.97v-.4L15.3 28.4c-.29-.34-.34-.5-.34-1.03v-8.95l6.13 13.36h.71l5.26-13.36v10.64c0 .3 0 .35-.19.53l-1.85 1.8v.4h9.2v-.4l-1.83-1.8c-.18-.18-.2-.24-.2-.53V15.94c0-.3.02-.35.2-.53l1.82-1.8v-.4h-6.47l-4.62 11.55-5.2-11.54h-6.8v.4l2.15 2.63c.24.3.29.37.29.77v10.35z"></path></svg></a></div></div><div class="n an s"><span class="ao b ap aq ar as n at au"><div class="ag ah"><a href="https://medium.com/search" class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk"><svg width="25" height="25" viewBox="0 0 25 25" class="bl bm n bn bo"><path d="M20.07 18.93l-4.16-4.15a6 6 0 1 0-.88.88l4.15 4.16a.62.62 0 1 0 .89-.89zM6.5 11a4.75 4.75 0 1 1 9.5 0 4.75 4.75 0 0 1-9.5 0z"></path></svg></a><div class="bm ag bo"><a href="https://medium.com/me/activity" class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk"><button class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk n"><svg width="25" height="25" viewBox="-293 409 25 25" class="hn n"><path d="M-273.33 423.67l-1.67-1.52v-3.65a5.5 5.5 0 0 0-6.04-5.47 5.66 5.66 0 0 0-4.96 5.71v3.41l-1.68 1.55a1 1 0 0 0-.32.74V427a1 1 0 0 0 1 1h3.49a3.08 3.08 0 0 0 3.01 2.45 3.08 3.08 0 0 0 3.01-2.45h3.49a1 1 0 0 0 1-1v-2.59a1 1 0 0 0-.33-.74zm-7.17 5.63c-.84 0-1.55-.55-1.81-1.3h3.62a1.92 1.92 0 0 1-1.81 1.3zm6.35-2.45h-12.7v-2.35l1.63-1.5c.24-.22.37-.53.37-.85v-3.41a4.51 4.51 0 0 1 3.92-4.57 4.35 4.35 0 0 1 4.78 4.33v3.65c0 .32.14.63.38.85l1.62 1.48v2.37z"></path></svg></button></a></div><div class="bm n g"><div><a href="https://medium.com/membership?source=upgrade_membership---nav_full------------------------" class="bp bq am br bs bt bu bv be bw ao b ap aq ar as bx by af bz ca bh">Upgrade</a></div></div><div class="ag"><div class="cb cc ag ah"><button class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk"><img alt="Sagar Kumar" src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/1_k0EpKE9wT5-E8uKkx8KIug.jpeg" class="n cd ce cf" width="32" height="32"></button></div></div></div></span></div></section></div><div class="cg n c ch"><section class="w x y z ci ac ae af cj ag ah"><div class="ck n an"><a href="https://medium.com/tensorflow"><div class="cl cm"><img alt="TensorFlow" src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/1_nhv3orDSO8ggJ6q6f-qu4Q.png" class="" width="35" height="36"></div></a></div><div class="cn n co"><ul class="cp bd cq cr cs ag ct g cu cv cw"><li class="ag ah cx cy cz da"><span class="ao dd de df at dg dh"><a href="https://medium.com/tensorflow/tagged/announcements" class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk">Announcements</a></span></li><li class="ag ah cx cy cz da"><span class="ao dd de df at dg dh"><a href="https://medium.com/tensorflow/tagged/keras" class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk">Keras</a></span></li><li class="ag ah cx cy cz da"><span class="ao dd de df at dg dh"><a href="https://medium.com/tensorflow/tagged/javascript" class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk">TensorFlow.js</a></span></li><li class="ag ah cx cy cz da"><span class="ao dd de df at dg dh"><a href="https://medium.com/tensorflow/tagged/mobile" class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk">Mobile</a></span></li><li class="ag ah cx cy cz da"><span class="ao dd de df at dg dh"><a href="https://medium.com/tensorflow/tagged/responsible-ai" class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk">Responsible AI</a></span></li><li class="ag ah cx cy cz da"><span class="ao dd de df at dg dh"><a href="https://medium.com/tensorflow/archive" class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk">All stories</a></span></li><span class="cx db dc"></span><li class="ag ah cx cy cz da"><span class="ao dd de df at dg dh"><a href="http://tensorflow.org/" class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk">tensorflow.org</a></span></li></ul></div></section></div></div></nav><div class="di dj n dk dl"></div><article><div class="dm dn n"><section class="w x y z ac do af n"></section></div><span class="n"></span><div><div class="gi q ho hp hq dq"></div><div class="y z do eu"><div class="n h g f e"><aside class="oo gi p" style="width: 294.5px;"><div class="or nw gi os cr"><h4 class="ao dd de aq at"><span class="bz nw cr cj eg">Top highlight</span></h4></div></aside></div></div><section class="hr hs ht hu hv"><div class="af hw ac do w x"><div><div id="6399" class="hx hy bq ap hz b ia ib ic id ie if ig"><h1 class="hz b ia ih bq">An Introduction to Biomedical Image Analysis with TensorFlow and DLTK</h1></div><div class="ii"><div class="ah ag"><div><a href="https://medium.com/@m_rajchl"><img alt="Martin Rajchl" src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/0_Rqz0XPiSgFjDvJ7w.jpg" class="n cd ij ik" width="48" height="48"></a></div><div class="il ac n"><div class="ag"><div style="flex:1"><span class="ao b ap aq ar as n bq am"><div class="im ag ah in" data-test-id="postByline"><span class="ao dd ee aq cj io eg eh ip ej bq"><a class="av aw ax ay az ba bb bc bd be iq bh bi bj bk" href="https://medium.com/@m_rajchl">Martin Rajchl</a></span><div class="ir n an h"><button class="is br ek el em en eo ep be bw ao b ap it de as bx by af bz ca bh">Follow</button></div></div></span></div></div><span class="ao b ap aq ar as n at au"><span class="ao dd ee aq cj io eg eh ip ej at"><div><a class="av aw ax ay az ba bb bc bd be iq bh bi bj bk" href="https://medium.com/tensorflow/an-introduction-to-biomedical-image-analysis-with-tensorflow-and-dltk-2c25304e7c13">Jul 4, 2018</a> <!-- -->·<!-- --> <!-- -->12<!-- --> min read</div></span></span></div></div></div></div><p id="64a0" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph="">By Martin Rajchl, S. Ira Ktena and Nick Pawlowski — Imperial College London</p><p id="1cc4" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph=""><a href="https://dltk.github.io/" class="av ca ji jj jk jl">DLTK</a>, the <em class="jm">Deep Learning Toolkit for Medical Imaging </em>extends <a href="https://www.tensorflow.org/" class="av ca ji jj jk jl">TensorFlow</a><em class="jm"> </em>to enable deep learning on biomedical images. It provides specialty ops and functions, implementations of models, <a href="https://github.com/DLTK/DLTK/tree/master/examples/tutorials" class="av ca ji jj jk jl">tutorials</a> (as used in this blog) and <a href="https://github.com/DLTK/DLTK/tree/master/examples/applications" class="av ca ji jj jk jl">code examples for typical applications</a>.</p><p id="1ad2" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph="">This blog post serves as a quick introduction to deep learning with biomedical images, where we will demonstrate a few issues and solutions to current engineering problems and show you how to get up and running with a prototype for your problem.</p><figure class="jn jo jp jq jr fe js y z paragraph-image"><div class="jv n eu jw"><div class="jx n"><div class="dp jt gi p q gh ac cj v ju"><img src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/1_pfe19FYqq9GKAUUgBBjXjg.jpeg" class="gi p q gh ac jy jz pf-large-image" width="700" height="350"></div><img class="my mz gi p q gh ac gf pf-large-image" width="700" height="350" src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/1_pfe19FYqq9GKAUUgBBjXjg(1).jpeg"><noscript><img src="https://miro.medium.com/max/1400/1*pfe19FYqq9GKAUUgBBjXjg.jpeg" class="gi p q gh ac" width="700" height="350"/></noscript></div></div><figcaption class="at ee ka kb hk do y z kc ao dd" data-selectable-paragraph="">website: <a href="https://dltk.github.io/" class="av ca ji jj jk jl">https://dltk.github.io</a>; source: <a href="https://github.com/DLTK/DLTK" class="av ca ji jj jk jl">https://github.com/DLTK/DLTK;</a></figcaption></figure><h1 id="ed31" class="kd ke bq ap ao ea kf kg kh ki kj kk kl km kn ko kp" data-selectable-paragraph="">Overview</h1><p id="89ad" class="iu iv bq ap iw b ix kq iz kr jb ks jd kt jf ku jh" data-selectable-paragraph=""><strong class="iw kv">What is biomedical image analysis and why is it needed?</strong> Biomedical images are measurements of the human body on different scales (i.e. microscopic, macroscopic, etc.). They come in a wide variety of imaging modalities (e.g. a CT scanner, an ultrasound machine, etc.) and measure a physical property of the human body (e.g. radiodensity, the opacity to X-rays). These images are interpreted by domain experts (e.g. a radiologist) for clinical tasks (e.g. a diagnosis) and have a large impact on decision making of physicians.</p><figure class="jn jo jp jq jr fe kw y z paragraph-image"><div class="jv n eu jw"><div class="kx n"><div class="dp jt gi p q gh ac cj v ju"><img src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/1_1bNAG7ujkPbx64HWXKMoeg.png" class="gi p q gh ac jy jz pf-large-image" width="700" height="376"></div><img class="my mz gi p q gh ac gf pf-large-image" width="700" height="376" src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/1_1bNAG7ujkPbx64HWXKMoeg(1).png"><noscript><img src="https://miro.medium.com/max/1400/1*1bNAG7ujkPbx64HWXKMoeg.png" class="gi p q gh ac" width="700" height="376"/></noscript></div></div><figcaption class="at ee ka kb hk do y z kc ao dd" data-selectable-paragraph="">Examples of medical images (from top left to bottom right): Multi-sequence brain MRI: T1-weighted, T1 inversion recovery and T2 FLAIR channels; Stitched whole-body MRI; planar cardiac ultrasound; chest X-ray; cardiac cine MRI.</figcaption></figure><p id="62bf" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph="">Biomedical images are typically volumetric images (3D) and sometimes have an additional time dimension (4D) and/or multiple channels (4-5D) (e.g. multi-sequence MR images). The variation in biomedical images is quite different from that of a natural image (e.g. a photograph), as clinical protocols aim to stratify how an image is acquired (e.g. a patient is lying on his/her back, the head is not tilted, etc.). In their analysis, we aim to detect subtle differences (i.e. some small region indicating an abnormal finding).</p><p id="9de5" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph=""><strong class="iw kv">Why computer vision and machine learning?</strong> <mark class="op oq ey">Computer vision methods have long been employed to automatically analyze biomedical images</mark>. The recent advent of deep learning has replaced many other machine learning methods, because it avoids the creation of hand-engineering features, thus removing a critical source of error from the process. Additionally, the fast inference speeds of GPU-accelerated fully networks, allows us scale analyses to unprecedented amounts of data (e.g. <a href="https://imaging.ukbiobank.ac.uk/" class="av ca ji jj jk jl">10⁶ subject images</a>).</p><p id="e096" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph=""><strong class="iw kv">Can we readily employ deep learning libraries for biomedical imaging?Why create DLTK? <br></strong>The main reasons for creating <a href="http://dltk.github.io/" class="av ca ji jj jk jl"><em class="jm">DLTK</em></a><em class="jm"> </em>were to include speciality tools for this domain out of the box<em class="jm">. </em>While many deep learning libraries expose low-level operations (e.g. tensor multiplications, etc.) to the developers, a lot of the higher-level specialty operations are missing for their use on volumetric images (e.g. differentiable 3D upsampling layers, etc.), and due to the additional spatial dimension(s) of the images, we can run into memory issues (e.g. storing a single copy of a database of 1k CT images, with image dimensions of 512x512x256 voxels in float32 is ~268 GB). Due to the different nature of acquisition, some images will require special pre-processing (e.g. intensity normalization, bias-field correction, de-noising, spatial normalization/registration, etc).</p><h1 id="f226" class="kd ke bq ap ao ea kf ky kh kz kj la kl lb kn lc kp" data-selectable-paragraph=""><strong class="bb">File formats, headers &amp; reading images</strong></h1><p id="0517" class="iu iv bq ap iw b ix kq iz kr jb ks jd kt jf ku jh" data-selectable-paragraph="">While many vendors of imaging modalities produce images in the <a href="https://en.wikipedia.org/wiki/DICOM" class="av ca ji jj jk jl"><em class="jm">DICOM</em></a> standard format, saving volumes in series of 2D slices, many analysis libraries rely on formats more suited for computing and interfacing with medical images. We use the <a href="https://nifti.nimh.nih.gov/nifti-1/" class="av ca ji jj jk jl"><em class="jm">NifTI </em></a>(or .nii format), originally developed for brain imaging, but widely used for most other volume images in both <em class="jm">DLTK </em>and for this tutorial. What this and other format saves is necessary information to reconstruct the image container and orient it in physical space.</p><p id="b694" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph="">For this, it requires specialty header information, and we will go through a few attributes to consider for deep learning:</p><ul class=""><li id="9bdf" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh ld le lf" data-selectable-paragraph="">Dimensions and size store information about how to reconstruct the image (e.g. a volume into three dimensions with a size vector).</li><li id="82dc" class="iu iv bq ap iw b ix lg iz lh jb li jd lj jf lk jh ld le lf" data-selectable-paragraph="">Data type</li><li id="a8d7" class="iu iv bq ap iw b ix lg iz lh jb li jd lj jf lk jh ld le lf" data-selectable-paragraph="">Voxel spacing (also the physical dimensions of voxels, typically in mm)</li><li id="7aa6" class="iu iv bq ap iw b ix lg iz lh jb li jd lj jf lk jh ld le lf" data-selectable-paragraph="">Physical coordinate system origin</li><li id="ae4d" class="iu iv bq ap iw b ix lg iz lh jb li jd lj jf lk jh ld le lf" data-selectable-paragraph="">Direction</li></ul><p id="394d" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph=""><strong class="iw kv">Why are these attributes important?</strong> The network will train in the space of voxels, meaning we will create tensors of shape and dimensions [batch_size, dx, dy, dz, channels/features] and feed it to the network. The network will train in that voxel space and assume that all images (also unseen test images) are normalised in that space or might have issues to generalise. In that voxel space, the feature extractors (e.g. convolutional layers) will assume that voxel dimensions are isotropic (i.e. are the same in each dimension) and all images are oriented the same way.</p><p id="75c4" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph="">However, since most images are depicting physical space, we need to transform from that physical space into a common voxel space:</p><p id="92d9" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph="">If all images are oriented the same way (sometimes we require registration to spatially normalize images: check out <a href="https://biomedia.doc.ic.ac.uk/software/mirtk/" class="av ca ji jj jk jl"><em class="jm">MIRTK</em></a>), we can compute the scaling transform from physical to voxel space via</p><pre class="jn jo jp jq jr hi fl cs"><span id="5628" class="ll ke bq ap lm b ee ln lo n lp" data-selectable-paragraph="">phys_coords = origin + voxel_spacing * voxel_coord</span></pre><p id="6237" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph="">where all these information are vectors stored in the .nii header.</p><p id="c51a" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph=""><strong class="iw kv">Reading .nii images:</strong> There are several libraries to read .nii files and access the header information and parse it to obtain a reconstructed image container as a <a href="http://www.numpy.org/" class="av ca ji jj jk jl"><em class="jm">numpy</em></a> array. We chose <a href="http://www.simpleitk.org/" class="av ca ji jj jk jl"><em class="jm">SimpleITK</em></a>, a python wrapper around the <a href="https://itk.org/" class="av ca ji jj jk jl"><em class="jm">ITK</em></a> library, which allows us to import additional image filters for pre-processing and other tasks:</p><figure class="jn jo jp jq jr"><div class="jv n eu"><div class="ng n"><iframe src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/936e646e1a7cb606254959e929ba899f.html" frameborder="0" height="279" width="680" title="" class="gi p q gh ac"></iframe></div></div></figure><h1 id="3ba3" class="kd ke bq ap ao ea kf kg kh ki kj kk kl km kn ko kp" data-selectable-paragraph=""><strong class="bb">Data I/O considerations</strong></h1><p id="d955" class="iu iv bq ap iw b ix kq iz kr jb ks jd kt jf ku jh" data-selectable-paragraph="">Depending on the size of the training database, there are several options to feed .nii image data into the network graph. Each of these methods has specific trade-offs in terms of speed and can be a bottleneck during training. We will go through and explain three options:</p><p id="08a2" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph=""><strong class="iw kv">In memory &amp; feeding dictionaries: </strong>We can create a <em class="jm">tf.placeholder</em> to the network graph and feed it via feed_dict during training. We read all .nii files from disk , process them in python (c.f. <em class="jm">load_data()</em>) and store all training examples in memory, where we feed from:</p><figure class="jn jo jp jq jr"><div class="jv n eu"><div class="nb n"><iframe src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/77c3ad5f0293bd66f0cd0b6fe3700e24.html" frameborder="0" height="675" width="680" title="" class="gi p q gh ac"></iframe></div></div></figure><p id="903a" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph="">TLDR: this direct approach is typically the fastest and easiest to implement, as it avoids continuously reading the data from disk, however requires to keep the entire database of training examples (and validation examples) in memory, which is not feasible for larger databases or larger image files.</p><p id="4332" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph=""><strong class="iw kv">Using a <em class="jm">TFRecords </em>database:</strong> For most deep learning problems on image volumes, the database of training examples is too large to fit into memory. The <em class="jm">TFRecords </em>format allows to serialise training examples and store them on disk with quick write access (i.e. parallel data reads):</p><figure class="jn jo jp jq jr"><div class="jv n eu"><div class="nb n"><iframe src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/2831ccde9f887e33e92dfe26d35a94b7.html" frameborder="0" height="675" width="680" title="" class="gi p q gh ac"></iframe></div></div></figure><p id="28d5" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph="">The format can directly interface with <em class="jm">TensorFlow </em>and can be directly integrated into a training loop in a tf.graph:</p><figure class="jn jo jp jq jr"><div class="jv n eu"><div class="nd n"><iframe src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/cd7b8657e8fbc58d578f32c8aa648777.html" frameborder="0" height="697" width="680" title="" class="gi p q gh ac"></iframe></div></div></figure><p id="47ac" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph="">TLDR<em class="jm">:</em> <em class="jm">TFRecords </em>are fast means of accessing files from disk, but require to store yet another copy of the entire training database. If we are aiming to work with a database of several TB size, this could be prohibitive.</p><p id="7b99" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph=""><strong class="iw kv">Using native python generators:</strong> Lastly, we can use python generators, creating a <em class="jm">read_fn()</em> to directly load the image data…</p><figure class="jn jo jp jq jr"><div class="jv n eu"><div class="ne n"><iframe src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/66d9755b4efc1fe6034cef4c133a0a93.html" frameborder="0" height="1269" width="680" title="" class="gi p q gh ac"></iframe></div></div></figure><p id="5cd0" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph="">and <em class="jm">tf.data.Dataset.from_generator()</em> to queue the examples:</p><figure class="jn jo jp jq jr"><div class="jv n eu"><div class="nc n"><iframe src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/0b91ef7382315937395647d97e30e8fa.html" frameborder="0" height="668" width="680" title="" class="gi p q gh ac"></iframe></div></div></figure><p id="9a58" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph="">TLDR<em class="jm">:</em> It avoids creating additional copies of the image database, however is considerably slower than <em class="jm">TFRecords</em>, due to the fact that the generator cannot parallel read and map functions.</p><p id="3743" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph=""><strong class="iw kv">Speed benchmarking &amp; choosing a method:</strong> We ran these three methods of reading .nii files to <em class="jm">TensorFlow </em>and compared the time required to load and feed a fixed-size example database. All codes and results can be found in here.</p><p id="8bf7" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph="">The obviously fastest method was feeding from memory via placeholders in 5.6 seconds, followed by <em class="jm">TFRecords </em>with 31.1 seconds and the un-optimised reading from disk using python generators with 123.5 seconds. However, as long as the forward/backward passes during training are the computational bottleneck, the speed of the data I/O is negligible.</p><h1 id="82cd" class="kd ke bq ap ao ea kf ky kh kz kj la kl lb kn lc kp" data-selectable-paragraph=""><strong class="bb">Data normalization</strong></h1><p id="0999" class="iu iv bq ap iw b ix kq iz kr jb ks jd kt jf ku jh" data-selectable-paragraph="">As with natural images, we can normalize biomedical image data, however the methods might slightly vary. The aim of normalization is to remove some variation in the data (e.g. different subject pose or differences in image contrast, etc.) that is known and so simplify the detection of subtle differences we are interested in instead (e.g. the presence of a pathology). Here, we will go over the most common forms of normalization:</p><p id="4eae" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph=""><strong class="iw kv">Normalization of voxel intensities: </strong>This form is highly dependent on the imaging modality, the data was acquired with. Typical <a href="https://github.com/DLTK/DLTK/blob/dev/dltk/io/preprocessing.py#L9" class="av ca ji jj jk jl">zero-mean, unit variance normalization</a> is standard for qualitative images (e.g. weighted brain MR images, where the contrast is highly dependent on acquisition parameters, typically set by an expert). If we employ such statistical approaches, we use statistics from a full single volume, rather than an entire database.</p><p id="3913" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph="">In contrast to this, quantitative imaging measures a physical quantity (e.g. radio-density in CT imaging, where the intensities are comparable across different scanners) and benefit from clipping and/or re-scaling, as <a href="https://github.com/DLTK/DLTK/blob/dev/dltk/io/preprocessing.py#L39" class="av ca ji jj jk jl">simple range normalisation</a> (e.g. to [-1,1]).</p><figure class="jn jo jp jq jr fe lr y z paragraph-image"><div class="jv n eu jw"><div class="ls n"><div class="my mz gi p q gh ac cj v ju"><img src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/1_UmKR5B3wM8mdhsnVsz01hg.png" class="gi p q gh ac jy jz" width="700" height="168"></div><img class="dp jt gi p q gh ac gf" width="700" height="168" src="https://medium.com/tensorflow/an-introduction-to-biomedical-image-analysis-with-tensorflow-and-dltk-2c25304e7c13"><noscript><img src="https://miro.medium.com/max/1400/1*UmKR5B3wM8mdhsnVsz01hg.png" class="gi p q gh ac" width="700" height="168"/></noscript></div></div><figcaption class="at ee ka kb hk do y z kc ao dd" data-selectable-paragraph="">Example intensity normalisation methods</figcaption></figure><p id="4210" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph=""><strong class="iw kv">Spatial normalisation: </strong>Normalising for image orientation avoids that the model will have to learn all possible orientations, which largely reduces the amount of training images required (see the importance of header attributes to know what orientation an image is in). We additionally account for voxel spacing, which may vary between images, even when acquired from the same scanner. This can be done by resampling to an isotropic resolution:</p><figure class="jn jo jp jq jr"><div class="jv n eu"><div class="nf n"><iframe src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/f8dccba717dd779181028e832beaf27b.html" frameborder="0" height="690" width="680" title="" class="gi p q gh ac"></iframe></div></div></figure><p id="48ac" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph="">If further normalisation is required, we can use medical image registration packages (e.g. <a href="https://biomedia.doc.ic.ac.uk/software/mirtk/" class="av ca ji jj jk jl"><em class="jm">MIRTK</em></a>, etc.) and register the images into the same space, so that voxel locations between images correspond to each other. A typical step in analysing structural brain MR images (e.g. T1-weighted MR images) is to register all images in the training database to a reference standard, such as a mean atlas (e.g. the <a href="https://www.mcgill.ca/bic/software/tools-data-analysis/anatomical-mri/atlases/mni-305" class="av ca ji jj jk jl"><em class="jm">MNI 305</em></a> atlas). Depending on the degrees of freedom of the registration method, this can also normalise for size (affine registration) or shape (deformable registration). These two variants are rather rarely used, as they remove some of the information in the image (i.e. shape information or size information), that might be important for analysis (e.g. a large heart might be predictive of heart disease).</p><h1 id="ea7b" class="kd ke bq ap ao ea kf ky kh kz kj la kl lb kn lc kp" data-selectable-paragraph="">Data augmentation</h1><p id="c0d3" class="iu iv bq ap iw b ix kq iz kr jb ks jd kt jf ku jh" data-selectable-paragraph="">More often than not, there is a limited amount of data available and some of the variation is not covered. A few examples include:</p><ul class=""><li id="dc38" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh ld le lf" data-selectable-paragraph="">soft-tissue organs, where a wide range of normal shapes exist</li><li id="d52f" class="iu iv bq ap iw b ix lg iz lh jb li jd lj jf lk jh ld le lf" data-selectable-paragraph="">pathologies, such as cancer lesions, which can largely vary in shape and location</li><li id="ab13" class="iu iv bq ap iw b ix lg iz lh jb li jd lj jf lk jh ld le lf" data-selectable-paragraph="">free-hand ultrasound images, where a lot of possible views are possible</li></ul><p id="a9ab" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph="">In order to properly generalise to unseen test cases, we augment training images by simulating a variation in the data we aim to be robust against. Similarly to normalisation methods, we distinguish between intensity and spatial augmentations:</p><p id="48c2" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph=""><em class="jm">Examples of intensity augmentations:</em></p><ul class=""><li id="2a2d" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh ld le lf" data-selectable-paragraph="">Adding noise to training images generalise to noisy images</li><li id="ef22" class="iu iv bq ap iw b ix lg iz lh jb li jd lj jf lk jh ld le lf" data-selectable-paragraph="">Adding a random offset or contrast to handle differences between images</li></ul><p id="6dc6" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph="">Examples of spatial augmentations:</p><ul class=""><li id="3fed" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh ld le lf" data-selectable-paragraph="">Flipping the image tensor in directions on where to expect symmetry (e.g. a left/right flip on brain scans)</li><li id="027c" class="iu iv bq ap iw b ix lg iz lh jb li jd lj jf lk jh ld le lf" data-selectable-paragraph="">Random deformations, (e.g. for mimicking differences in organ shape)</li><li id="ee18" class="iu iv bq ap iw b ix lg iz lh jb li jd lj jf lk jh ld le lf" data-selectable-paragraph="">Rotations along axes (e.g. for simulating difference ultrasound view angles)</li><li id="808a" class="iu iv bq ap iw b ix lg iz lh jb li jd lj jf lk jh ld le lf" data-selectable-paragraph="">Random cropping and training on patches</li></ul><figure class="jn jo jp jq jr fe lt y z paragraph-image"><div class="jv n eu jw"><div class="lu n"><div class="my mz gi p q gh ac cj v ju"><img src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/1_FgV1en0rLz5UFGQzdFdqvg.png" class="gi p q gh ac jy jz pf-large-image" width="700" height="446"></div><img class="dp jt gi p q gh ac gf pf-large-image" width="700" height="446" src="https://medium.com/tensorflow/an-introduction-to-biomedical-image-analysis-with-tensorflow-and-dltk-2c25304e7c13"><noscript><img src="https://miro.medium.com/max/1400/1*FgV1en0rLz5UFGQzdFdqvg.png" class="gi p q gh ac" width="700" height="446"/></noscript></div></div><figcaption class="at ee ka kb hk do y z kc ao dd" data-selectable-paragraph="">Example intensity and spatial augmentation techniques</figcaption></figure><p id="3558" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph=""><em class="jm">Important notes on augmentation and data I/O:</em> Depending on which augmentations are required or helpful, some operations are only available in python (e.g. <a href="https://github.com/DLTK/DLTK/blob/master/dltk/io/augmentation.py#L75" class="av ca ji jj jk jl">random deformations</a>), meaning that if a reading method is used that uses raw <em class="jm">TensorFlow </em>(i.e. <em class="jm">TFRecords </em>or <em class="jm">tf.placeholder</em>), they will need to be pre-computed and stored to disk, thus largely increasing the size of the training database.</p><h1 id="4ea2" class="kd ke bq ap ao ea kf ky kh kz kj la kl lb kn lc kp" data-selectable-paragraph="">Class balancing</h1><p id="3e88" class="iu iv bq ap iw b ix kq iz kr jb ks jd kt jf ku jh" data-selectable-paragraph="">Domain expert interpretations (e.g. manual segmentations or disease classes) are a requirement during supervised learning from medical images. Typically, the image-level (e.g. a disease class) or voxel-level (i.e. segmentation) labels are not available in the same ratio, which means that the network will not see an equal amount of examples from each class during training. This does not have a large effect on accuracy if the class ratios are somewhat similar (e.g. 30/70 for a binary classification case). However, since most losses are average costs on the entire batch, the network will first learn to correctly predict the most frequently seen class (e.g. background or normal cases, which are are typically more examples available of).</p><p id="57ca" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph="">A class imbalance during training will have a larger impact on rare phenomena (e.g. small lesions in image segmentation) and largely impact the test accuracy.</p><p id="dc22" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph="">To avoid this drop, there are two typical approaches to combat class imbalances in datasets:</p><ul class=""><li id="6e12" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh ld le lf" data-selectable-paragraph="">Class balancing via sampling: Here, we aim to correct the frequencies of seen examples during sampling. This can be done by a) sampling an equal amount from each class, b) under-sampling over-represented classes or c) over-sampling less frequent classes. In <em class="jm">DLTK</em>, we have an implementation for a), which can be found <a href="https://github.com/DLTK/DLTK/blob/blog/dltk/io/augmentation.py#L120" class="av ca ji jj jk jl">here</a>. We sample random locations in the image volume and consider an extracted example, if it contains the class we are looking for.</li><li id="6913" class="iu iv bq ap iw b ix lg iz lh jb li jd lj jf lk jh ld le lf" data-selectable-paragraph="">Class balancing via loss function: In contrast to typical voxel-wise mean losses (e.g. categorical cross-entropy, L2, etc.), we can a) use a loss function that is inherently balanced (e.g. <a href="https://github.com/DLTK/DLTK/blob/master/dltk/core/losses.py#L51" class="av ca ji jj jk jl">smooth Dice loss</a>, which is a mean Dice-coefficient across all classes) or b) <a href="https://github.com/DLTK/DLTK/blob/master/dltk/core/losses.py#L10" class="av ca ji jj jk jl">re-weight the losses for each prediction by the class frequency</a> (e.g. median-frequency re-weighted cross-entropy).</li></ul><h1 id="530c" class="kd ke bq ap ao ea kf ky kh kz kj la kl lb kn lc kp" data-selectable-paragraph="">Example application highlights</h1><p id="f375" class="iu iv bq ap iw b ix kq iz kr jb ks jd kt jf ku jh" data-selectable-paragraph="">With all the basic knowledge provided in this blog post, we can now look into building full applications for deep learning on medical images with <em class="jm">TensorFlow</em>. We have implemented several typical applications using deep neural networks and will walk through a few of them to give you an insight on what problems you now can attempt to tackle.</p><p id="be10" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph=""><em class="jm">Note: These example applications learn something meaningful, but were built for demo purposes, rather than high-performance implementations.</em></p><h2 id="2f0f" class="ll ke bq ap ao ea lv lw lx ly lz ma mb mc md me mf" data-selectable-paragraph=""><strong class="bb">Example datasets</strong></h2><p id="949c" class="iu iv bq ap iw b ix kq iz kr jb ks jd kt jf ku jh" data-selectable-paragraph="">We provide <a href="https://github.com/DLTK/DLTK/tree/master/data" class="av ca ji jj jk jl">download and pre-processing scripts</a> for all the examples below. For most cases (including the demos above), we used the <a href="http://brain-development.org/ixi-dataset/" class="av ca ji jj jk jl"><em class="jm">IXI</em> brain database</a>. For image segmentation, we downloaded the <a href="http://mrbrains13.isi.uu.nl/" class="av ca ji jj jk jl"><em class="jm">MRBrainS13</em> challenge database</a>, which you will need to register for, before you can download it.</p><h2 id="332c" class="ll ke bq ap ao ea lv lw lx ly lz ma mb mc md me mf" data-selectable-paragraph="">Image segmentation of multi-channel brain MR images</h2><figure class="jn jo jp jq jr fe mg y z paragraph-image"><div class="jv n eu jw"><div class="mh n"><div class="my mz gi p q gh ac cj v ju"><img src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/1_OjngIU6_yw_cIT-b8ConDA.png" class="gi p q gh ac jy jz pf-large-image" width="700" height="543"></div><img class="dp jt gi p q gh ac gf pf-large-image" width="700" height="543" src="https://medium.com/tensorflow/an-introduction-to-biomedical-image-analysis-with-tensorflow-and-dltk-2c25304e7c13"><noscript><img src="https://miro.medium.com/max/1400/1*OjngIU6_yw_cIT-b8ConDA.png" class="gi p q gh ac" width="700" height="543"/></noscript></div></div><figcaption class="at ee ka kb hk do y z kc ao dd" data-selectable-paragraph="">Tensorboard visualisation of multi-sequence image inputs, target labels and predictions</figcaption></figure><p id="2954" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph="">This image segmentation application learns to predict brain tissues and white matter lesions from multi-sequence MR images (T1-weighted, T1 inversion recovery and T2 FLAIR) on the small (N=5) <em class="jm">MRBrainS </em>challenge dataset. It uses a 3D U-Net-like network with residual units as feature extractors and tracks the Dice coefficient accuracy for each label in <em class="jm">TensorBoard</em>.</p><p id="8bb0" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph="">The code and instructions can be found <a href="https://github.com/DLTK/DLTK/tree/master/examples/applications/MRBrainS13_tissue_segmentation" class="av ca ji jj jk jl">here</a>.</p><h2 id="e98a" class="ll ke bq ap ao ea lv lw lx ly lz ma mb mc md me mf" data-selectable-paragraph=""><strong class="bb">Age regression and sex classification from T1-weighted brain MR images</strong></h2><figure class="jn jo jp jq jr fe mi y z paragraph-image"><div class="jv n eu jw"><div class="mj n"><div class="my mz gi p q gh ac cj v ju"><img src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/1_s81FfZex1tzjAoxv7mSSYQ.png" class="gi p q gh ac jy jz pf-large-image" width="700" height="409"></div><img class="dp jt gi p q gh ac gf pf-large-image" width="700" height="409" src="https://medium.com/tensorflow/an-introduction-to-biomedical-image-analysis-with-tensorflow-and-dltk-2c25304e7c13"><noscript><img src="https://miro.medium.com/max/1400/1*s81FfZex1tzjAoxv7mSSYQ.png" class="gi p q gh ac" width="700" height="409"/></noscript></div></div><figcaption class="at ee ka kb hk do y z kc ao dd" data-selectable-paragraph="">Example input T1-weighted brain MR images for regression and classification</figcaption></figure><p id="971e" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph="">Two similar applications employing a scalable 3D ResNet architecture learn to predict the subject’s age (regression) or the subject’s sex (classification) from T1–weighted brain MR images from the <em class="jm">IXI </em>database. The main difference between this applications is the loss function: While we train the regression network to predict the age as a continuous variable with a L2-loss (the mean squared differences between the predicted age and the real age), we use a categorical cross-entropy loss to predict the class of the sex.</p><p id="1404" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph="">The code and instructions for these applications can be found here: <a href="https://github.com/DLTK/DLTK/tree/master/examples/applications/IXI_HH_sex_classification_resnet" class="av ca ji jj jk jl">classification</a>, <a href="https://github.com/DLTK/DLTK/tree/master/examples/applications/IXI_HH_age_regression_resnet" class="av ca ji jj jk jl">regression</a>.</p><h2 id="0a81" class="ll ke bq ap ao ea lv lw lx ly lz ma mb mc md me mf" data-selectable-paragraph=""><strong class="bb">Representation learning on 3T multi-channel brain MR images</strong></h2><figure class="jn jo jp jq jr fe mk y z paragraph-image"><div class="jv n eu jw"><div class="ml n"><div class="my mz gi p q gh ac cj v ju"><img src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/1_lahtOexzm9GhpsHFYFMnPw.png" class="gi p q gh ac jy jz pf-large-image" width="700" height="465"></div><img class="dp jt gi p q gh ac gf pf-large-image" width="700" height="465" src="https://medium.com/tensorflow/an-introduction-to-biomedical-image-analysis-with-tensorflow-and-dltk-2c25304e7c13"><noscript><img src="https://miro.medium.com/max/1400/1*lahtOexzm9GhpsHFYFMnPw.png" class="gi p q gh ac" width="700" height="465"/></noscript></div></div><figcaption class="at ee ka kb hk do y z kc ao dd" data-selectable-paragraph="">Test images and reconstructions using a deep convolutional auto-encoder network</figcaption></figure><p id="69f2" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph="">Here we demo the use of a deep convolutional autoencoder architecture, a powerful tool for representation learning: The network takes a multi-sequence MR image as input and aims to reconstruct them. By doing so, it compresses the information of the entire training database in its latent variables. The trained weights can also be used for transfer learning or information compression. Note, that the reconstructed images are very smooth: This might be due to the fact that this application uses an L2-loss function or the network being to small to properly encode detailed information.</p><p id="7df4" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph="">The code and instructions can be found <a href="https://github.com/DLTK/DLTK/tree/master/examples/applications/IXI_HH_representation_learning_cae" class="av ca ji jj jk jl">here</a>.</p><h2 id="a00e" class="ll ke bq ap ao ea lv lw lx ly lz ma mb mc md me mf" data-selectable-paragraph="">Simple image super-resolution on T1w brain MR images</h2><figure class="jn jo jp jq jr fe mm y z paragraph-image"><div class="jv n eu jw"><div class="mn n"><div class="my mz gi p q gh ac cj v ju"><img src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/1_K1YrnwlXdgY15zms5VS01A.png" class="gi p q gh ac jy jz pf-large-image" width="700" height="211"></div><img class="dp jt gi p q gh ac gf pf-large-image" width="700" height="211" src="https://medium.com/tensorflow/an-introduction-to-biomedical-image-analysis-with-tensorflow-and-dltk-2c25304e7c13"><noscript><img src="https://miro.medium.com/max/1400/1*K1YrnwlXdgY15zms5VS01A.png" class="gi p q gh ac" width="700" height="211"/></noscript></div></div><figcaption class="at ee ka kb hk do y z kc ao dd" data-selectable-paragraph="">Image super-resolution: original target image; downsampled input image; linear upsampled image; predicted super-resolution;</figcaption></figure><p id="3a84" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph="">Single image super-resolution aims to learn how to upsample and reconstruct high-resolution images from low resolution inputs. This simple implementation creates a low-resolution version of an image and the super-res network learns to upsample the image to its original resolution (here the up-sampling factor is [4,4,4]). Additionally, we compute a linearly upsampled version to show the difference to the reconstructed image.</p><p id="4632" class="iu iv bq ap iw b ix iy iz ja jb jc jd je jf jg jh" data-selectable-paragraph="">The code and instructions can be found <a href="https://github.com/DLTK/DLTK/tree/master/examples/applications/IXI_HH_superresolution" class="av ca ji jj jk jl">here</a>.</p><h1 id="33b7" class="kd ke bq ap ao ea kf ky kh kz kj la kl lb kn lc kp" data-selectable-paragraph=""><strong class="bb">Lastly…</strong></h1><p id="a201" class="iu iv bq ap iw b ix kq iz kr jb ks jd kt jf ku jh" data-selectable-paragraph="">We hope that this tutorial has helped you to ease into the topic of deep learning on biomedical images. If you found it helpful, we appreciate you sharing it and following <em class="jm">DLTK </em>on <a href="https://github.com/DLTK/DLTK" class="av ca ji jj jk jl"><em class="jm">github</em></a>. If you require help with a similar problem, come to our <a href="https://gitter.im/DLTK/DLTK" class="av ca ji jj jk jl"><em class="jm">gitter.io</em> chat</a> and ask us. Maybe some day we can host your application in the <em class="jm">DLTK </em><a href="https://github.com/DLTK/models" class="av ca ji jj jk jl">model zoo</a>. Thanks for reading!</p><figure class="jn jo jp jq jr fe mo y z paragraph-image"><div class="jv n eu jw"><div class="mp n"><div class="my mz gi p q gh ac cj v ju"><img src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/1_iIbVhflH6hX-DCcQlJzZjw.png" class="gi p q gh ac jy jz pf-large-image" width="700" height="237"></div><img class="dp jt gi p q gh ac gf pf-large-image" width="700" height="237" src="https://medium.com/tensorflow/an-introduction-to-biomedical-image-analysis-with-tensorflow-and-dltk-2c25304e7c13"><noscript><img src="https://miro.medium.com/max/1400/1*iIbVhflH6hX-DCcQlJzZjw.png" class="gi p q gh ac" width="700" height="237"/></noscript></div></div><figcaption class="at ee ka kb hk do y z kc ao dd" data-selectable-paragraph=""><a href="https://twitter.com/dltk_" class="av ca ji jj jk jl">https://twitter.com/dltk_;</a> <a href="https://dltk.github.io/" class="av ca ji jj jk jl">https://dltk.github.io;</a> <a href="https://gitter.im/DLTK/DLTK" class="av ca ji jj jk jl">https://gitter.im/DLTK/DLTK;</a></figcaption></figure></div></section><hr class="mq dd ha mr ms hk mt mu mv mw mx"><section class="hr hs ht hu hv"><div class="af hw ac do w x"><h1 id="edc8" class="kd ke bq ap ao ea kf kg kh ki kj kk kl km kn ko kp" data-selectable-paragraph="">Resources</h1><p id="e80d" class="iu iv bq ap iw b ix kq iz kr jb ks jd kt jf ku jh" data-selectable-paragraph=""><a href="https://github.com/DLTK/DLTK/tree/master/examples/tutorials" class="av ca ji jj jk jl">Tutorial code</a>, <a href="https://github.com/DLTK/DLTK/tree/master/examples/applications" class="av ca ji jj jk jl">example applications</a>, <a href="https://github.com/DLTK/DLTK" class="av ca ji jj jk jl">DLTK source</a></p></div></section></div></article><div class="dp dq dr o ds dt du dv dw e" data-test-id="post-sidebar"><div class="ag dx"><div class="dy dz n"><a href="https://medium.com/tensorflow?source=post_sidebar--------------------------post_sidebar-" class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk"><h2 class="ao ea eb aq bq">TensorFlow</h2></a><div class="ec ed n"><h4 class="ao dd ee aq cj ef eg eh ei ej at">TensorFlow is an end-to-end open source platform for machine learning.</h4></div><div class="bz"><button class="bp br ek el em en eo ep be bw ao b ap aq ar as bx by af bz ca bh">Follow</button></div></div><div class="eq er es ag"><div class="ag ah"><div class="et n eu"><div class=""><button class="bc ev ew ex ey ez fa el"><svg width="29" height="29"><g fill-rule="evenodd"><path d="M13.74 1l.76 2.97.76-2.97zM16.82 4.78l1.84-2.56-1.43-.47zM10.38 2.22l1.84 2.56-.41-3.03zM22.38 22.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M9.1 22.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L6.1 15.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L6.4 11.26l-1.18-1.18a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L11.96 14a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L8.43 9.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L20.63 15c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM13 6.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 23 23.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="fb n"><div class="fc"><h4 class="ao dd ee aq at"><button class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk">1.2K </button></h4></div></div></div></div><div class="fd"><div><div class="bz"><button class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div><div><div class="dn fe ag dx ff"><section class="w x y z ac do af n"><div class="ec fg n"><h4 class="ao dd de aq at">Thanks to <!-- -->Nick Pawlowski<!-- -->.</h4></div><ul class="bc bd"><li class="bz cp fh fi"><a href="https://medium.com/tag/deep-learning" class="fj fk ca at n fl fm a b de">Deep Learning</a></li><li class="bz cp fh fi"><a href="https://medium.com/tag/tensorflow" class="fj fk ca at n fl fm a b de">TensorFlow</a></li><li class="bz cp fh fi"><a href="https://medium.com/tag/dltk" class="fj fk ca at n fl fm a b de">Dltk</a></li><li class="bz cp fh fi"><a href="https://medium.com/tag/medical-imaging" class="fj fk ca at n fl fm a b de">Medical Imaging</a></li></ul><div class="ag fn fo"><div class="ag ah"><div class="bm n eu"><div class=""><div class="c fp cd ag ah fq eu fr fs ep ft fu fv fw fx fy fz ga gb gc gd"><button class="bc ev ew ex ey ez ge ah gf cd ag el ff gg q gh gi p ac"><svg width="33" height="33" viewBox="0 0 33 33"><path d="M28.86 17.34l-3.64-6.4c-.3-.43-.71-.73-1.16-.8a1.12 1.12 0 0 0-.9.21c-.62.5-.73 1.18-.32 2.06l1.22 2.6 1.4 2.45c2.23 4.09 1.51 8-2.15 11.66a9.6 9.6 0 0 1-.8.71 6.53 6.53 0 0 0 4.3-2.1c3.82-3.82 3.57-7.87 2.05-10.39zm-6.25 11.08c3.35-3.35 4-6.78 1.98-10.47L21.2 12c-.3-.43-.71-.72-1.16-.8a1.12 1.12 0 0 0-.9.22c-.62.49-.74 1.18-.32 2.06l1.72 3.63a.5.5 0 0 1-.81.57l-8.91-8.9a1.33 1.33 0 0 0-1.89 1.88l5.3 5.3a.5.5 0 0 1-.71.7l-5.3-5.3-1.49-1.49c-.5-.5-1.38-.5-1.88 0a1.34 1.34 0 0 0 0 1.89l1.49 1.5 5.3 5.28a.5.5 0 0 1-.36.86.5.5 0 0 1-.36-.15l-5.29-5.29a1.34 1.34 0 0 0-1.88 0 1.34 1.34 0 0 0 0 1.89l2.23 2.23L9.3 21.4a.5.5 0 0 1-.36.85.5.5 0 0 1-.35-.14l-3.32-3.33a1.33 1.33 0 0 0-1.89 0 1.32 1.32 0 0 0-.39.95c0 .35.14.69.4.94l6.39 6.4c3.53 3.53 8.86 5.3 12.82 1.35zM12.73 9.26l5.68 5.68-.49-1.04c-.52-1.1-.43-2.13.22-2.89l-3.3-3.3a1.34 1.34 0 0 0-1.88 0 1.33 1.33 0 0 0-.4.94c0 .22.07.42.17.61zm14.79 19.18a7.46 7.46 0 0 1-6.41 2.31 7.92 7.92 0 0 1-3.67.9c-3.05 0-6.12-1.63-8.36-3.88l-6.4-6.4A2.31 2.31 0 0 1 2 19.72a2.33 2.33 0 0 1 1.92-2.3l-.87-.87a2.34 2.34 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.64l-.14-.14a2.34 2.34 0 0 1 0-3.3 2.39 2.39 0 0 1 3.3 0l.14.14a2.33 2.33 0 0 1 3.95-1.24l.09.09c.09-.42.29-.83.62-1.16a2.34 2.34 0 0 1 3.3 0l3.38 3.39a2.17 2.17 0 0 1 1.27-.17c.54.08 1.03.35 1.45.76.1-.55.41-1.03.9-1.42a2.12 2.12 0 0 1 1.67-.4 2.8 2.8 0 0 1 1.85 1.25l3.65 6.43c1.7 2.83 2.03 7.37-2.2 11.6zM13.22.48l-1.92.89 2.37 2.83-.45-3.72zm8.48.88L19.78.5l-.44 3.7 2.36-2.84zM16.5 3.3L15.48 0h2.04L16.5 3.3z" fill-rule="evenodd"></path></svg></button></div></div></div><div class="fb n"><div class="fc"><h4 class="ao dd ee aq bq"><button class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk">1.2K claps</button></h4></div></div></div><div class="ag ah"><div class="gj n an g"><a href="https://medium.com/p/2c25304e7c13/share/twitter?source=follow_footer--------------------------follow_footer-" class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk"><svg width="29" height="29" class="am"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="gj n an g"><a href="https://medium.com/p/2c25304e7c13/share/facebook?source=follow_footer--------------------------follow_footer-" class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk"><svg width="29" height="29" class="am"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="gj gk ch hidden-originally"><div class="bz"><button class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk"><svg width="25" height="25" class="am"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></button></div></div><div class="gj n an"><div class="fd"><div><div class="bz"><button class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div><div class="bz"><div class="n an"><button class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk"><svg width="25" height="25" viewBox="-480.5 272.5 21 21" class="am"><path d="M-463 284.6c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5z"></path></svg></button></div></div></div></div><div class="gl gm gn go"><div class="gp gq n eu"><span class="n gr aj gs"><div class="n gi gt gu"><a href="https://medium.com/@m_rajchl?source=follow_footer--------------------------follow_footer-"><img alt="Martin Rajchl" src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/0_Rqz0XPiSgFjDvJ7w(1).jpg" class="n cd cw gv" width="80" height="80"></a></div><span class="n"><div class="gw n gx"><p class="ao dd de aq at dg gy">Written by</p></div><div class="gw gz ag gx"><div class="ac ag ah fn"><h2 class="ao ea ha hb bq"><a class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk" href="https://medium.com/@m_rajchl?source=follow_footer--------------------------follow_footer-">Martin Rajchl</a></h2><div class="n g"><button class="bp br ek el em en eo ep be bw ao b ap aq ar as bx by af bz ca bh">Follow</button></div></div></div></span></span><div class="gw hc n gx ch"><div class="hd n"><h4 class="ao dd eb he at">Imperial College Research Fellow; Computing &amp; Brain Sci; Machine Learning, Computer Vision, Medical Imaging; http://dltk.github.io; Own views; Vienna;</h4></div><div class="gk hf ch hidden-originally"><button class="bp br ek el em en eo ep be bw ao b ap aq ar as bx by af bz ca bh">Follow</button></div></div></div><div class="gn n"></div><div class="gp gq n eu"><span class="n gr aj gs"><div class="n gi gt gu"><a href="https://medium.com/tensorflow?source=follow_footer--------------------------follow_footer-"><img alt="TensorFlow" src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/1_iDQvKoz7gGHc6YXqvqWWZQ.png" class="bw gv cw" width="80" height="80"></a></div><span class="n"><div class="gw gz ag gx"><div class="ac ag ah fn"><h2 class="ao ea ha hb bq"><a href="https://medium.com/tensorflow?source=follow_footer--------------------------follow_footer-" class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk">TensorFlow</a></h2><div class="n g"><div class="bz"><button class="bp br ek el em en eo ep be bw ao b ap aq ar as bx by af bz ca bh">Follow</button></div></div></div></div></span></span><div class="gw hg n gx ch"><div class="hd n"><h4 class="ao dd eb he at">TensorFlow is an end-to-end open source platform for machine learning.</h4></div><div class="gk hf ch hidden-originally"><div class="bz"><button class="bp br ek el em en eo ep be bw ao b ap aq ar as bx by af bz ca bh">Follow</button></div></div></div></div></div><div class="hh cg n"><a href="https://medium.com/p/2c25304e7c13/responses/show?source=follow_footer--------------------------follow_footer-" class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk"><div class="hi hj fj n hk ch"><span class="ek">See responses (9)</span></div></a></div></section><div class="hl n hm"><section class="w x y z ac ae af n"><section class="y ii z ac nh af ag fn"><section class="gp y z ac nh af n"><div class="ni dz gp n"><h2 class="ao ea nj nk bq">More From Medium</h2></div><div class="ag nl"><div class="nm nn n ak no"><div class="ac gh"><div class="gp ag dx"><div class="np n"><h4 class="ao dd ee aq at">More from TensorFlow</h4></div><div class="nq n"><a class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk n" href="https://medium.com/tensorflow/introducing-tensorflow-js-machine-learning-in-javascript-bf3eab376db?source=post_recirc---------0------------------"><div class="nr eu"><div class="gh gi ac"><div class="ns n nt nu gh ac nv nw"></div></div></div></a></div><div class="nq n"><a href="https://medium.com/tensorflow/introducing-tensorflow-js-machine-learning-in-javascript-bf3eab376db?source=post_recirc---------0------------------"><h3 class="bq am hz nx ap ny nz oa">Introducing TensorFlow.js: Machine Learning in Javascript</h3></a></div><div class="ag ah fn"><div class="ob n co"><div class="ah ag"><div><a href="https://medium.com/@tensorflow?source=post_recirc---------0------------------"><div class="eu oc od"><svg width="46" height="50" viewBox="0 0 46 50" class="oe gi of og oh oi dq"><path d="M1.45 15.22C5.43 7.07 13.59 1.5 23 1.5v-1C13.18.5 4.69 6.32.55 14.78l.9.44zM23 1.5c9.4 0 17.57 5.57 21.55 13.72l.9-.44C41.3 6.32 32.82.5 23 .5v1zm21.55 33.28C40.57 42.93 32.41 48.5 23 48.5v1c9.82 0 18.31-5.82 22.45-14.28l-.9-.44zM23 48.5c-9.4 0-17.57-5.57-21.55-13.72l-.9.44C4.7 43.68 13.18 49.5 23 49.5v-1z"></path></svg><img alt="TensorFlow" src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/1_iDQvKoz7gGHc6YXqvqWWZQ(1).png" class="n cd od oc" width="40" height="40"></div></a></div><div class="il ac n"><div class="ag"><div style="flex: 1 1 0%;"><span class="ao b ap aq ar as n bq am"><div class="di ag ah in" data-test-id="postByline"><span class="ao dd ee aq cj io eg eh ip ej bq"><a class="av aw ax ay az ba bb bc bd be iq bh bi bj bk" href="https://medium.com/@tensorflow?source=post_recirc---------0------------------">TensorFlow</a><span> in <a href="https://medium.com/tensorflow?source=post_recirc---------0------------------" class="av aw ax ay az ba bb bc bd be iq bh bi bj bk">TensorFlow</a></span></span></div></span></div></div><span class="ao b ap aq ar as n at au"><span class="ao dd ee aq cj io eg eh ip ej at"><div><a class="av aw ax ay az ba bb bc bd be iq bh bi bj bk" href="https://medium.com/tensorflow/introducing-tensorflow-js-machine-learning-in-javascript-bf3eab376db?source=post_recirc---------0------------------">Mar 30, 2018</a> · 4 min read</div></span></span></div></div></div><div class="ag ah"><div class="ag ah"><div class="et n eu"><div class=""><button class="bc ev ew ex ey ez fa fd"><svg width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="fb n"><div class="fc"><h4 class="ao dd ee aq at">14.7K </h4></div></div></div><div class="oj il ob cx ok n"></div><div class="fd"><div><div class="bz"><button class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div><div class="nm nn n ak no"><div class="ac gh"><div class="gp ag dx"><div class="np n"><h4 class="ao dd ee aq at">More from TensorFlow</h4></div><div class="nq n"><a class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk n" href="https://medium.com/tensorflow/neural-style-transfer-creating-art-with-deep-learning-using-tf-keras-and-eager-execution-7d541ac31398?source=post_recirc---------1------------------"><div class="nr eu"><div class="gh gi ac"><div class="ol n nt nu gh ac nv nw"></div></div></div></a></div><div class="nq n"><a href="https://medium.com/tensorflow/neural-style-transfer-creating-art-with-deep-learning-using-tf-keras-and-eager-execution-7d541ac31398?source=post_recirc---------1------------------"><h3 class="bq am hz nx ap ny nz oa">Neural Style Transfer: Creating Art with Deep Learning using tf.keras and eager execution</h3></a></div><div class="ag ah fn"><div class="ob n co"><div class="ah ag"><div><a href="https://medium.com/@tensorflow?source=post_recirc---------1------------------"><div class="eu oc od"><svg width="46" height="50" viewBox="0 0 46 50" class="oe gi of og oh oi dq"><path d="M1.45 15.22C5.43 7.07 13.59 1.5 23 1.5v-1C13.18.5 4.69 6.32.55 14.78l.9.44zM23 1.5c9.4 0 17.57 5.57 21.55 13.72l.9-.44C41.3 6.32 32.82.5 23 .5v1zm21.55 33.28C40.57 42.93 32.41 48.5 23 48.5v1c9.82 0 18.31-5.82 22.45-14.28l-.9-.44zM23 48.5c-9.4 0-17.57-5.57-21.55-13.72l-.9.44C4.7 43.68 13.18 49.5 23 49.5v-1z"></path></svg><img alt="TensorFlow" src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/1_iDQvKoz7gGHc6YXqvqWWZQ(1).png" class="n cd od oc" width="40" height="40"></div></a></div><div class="il ac n"><div class="ag"><div style="flex: 1 1 0%;"><span class="ao b ap aq ar as n bq am"><div class="di ag ah in" data-test-id="postByline"><span class="ao dd ee aq cj io eg eh ip ej bq"><a class="av aw ax ay az ba bb bc bd be iq bh bi bj bk" href="https://medium.com/@tensorflow?source=post_recirc---------1------------------">TensorFlow</a><span> in <a href="https://medium.com/tensorflow?source=post_recirc---------1------------------" class="av aw ax ay az ba bb bc bd be iq bh bi bj bk">TensorFlow</a></span></span></div></span></div></div><span class="ao b ap aq ar as n at au"><span class="ao dd ee aq cj io eg eh ip ej at"><div><a class="av aw ax ay az ba bb bc bd be iq bh bi bj bk" href="https://medium.com/tensorflow/neural-style-transfer-creating-art-with-deep-learning-using-tf-keras-and-eager-execution-7d541ac31398?source=post_recirc---------1------------------">Aug 3, 2018</a> · 10 min read</div></span></span></div></div></div><div class="ag ah"><div class="ag ah"><div class="et n eu"><div class=""><button class="bc ev ew ex ey ez fa fd"><svg width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="fb n"><div class="fc"><h4 class="ao dd ee aq at">4.8K </h4></div></div></div><div class="oj il ob cx ok n"></div><div class="fd"><div><div class="bz"><button class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div><div class="om nn n ak no"><div class="ac gh"><div class="gp ag dx"><div class="np n"><h4 class="ao dd ee aq at">More from TensorFlow</h4></div><div class="nq n"><a class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk n" href="https://medium.com/tensorflow/tf-jam-shooting-hoops-with-machine-learning-7a96e1236c32?source=post_recirc---------2------------------"><div class="nr eu"><div class="gh gi ac"><div class="on n nt nu gh ac nv nw"></div></div></div></a></div><div class="nq n"><a href="https://medium.com/tensorflow/tf-jam-shooting-hoops-with-machine-learning-7a96e1236c32?source=post_recirc---------2------------------"><h3 class="bq am hz nx ap ny nz oa">TF Jam — Shooting Hoops with Machine Learning</h3></a></div><div class="ag ah fn"><div class="ob n co"><div class="ah ag"><div><a href="https://medium.com/@tensorflow?source=post_recirc---------2------------------"><div class="eu oc od"><svg width="46" height="50" viewBox="0 0 46 50" class="oe gi of og oh oi dq"><path d="M1.45 15.22C5.43 7.07 13.59 1.5 23 1.5v-1C13.18.5 4.69 6.32.55 14.78l.9.44zM23 1.5c9.4 0 17.57 5.57 21.55 13.72l.9-.44C41.3 6.32 32.82.5 23 .5v1zm21.55 33.28C40.57 42.93 32.41 48.5 23 48.5v1c9.82 0 18.31-5.82 22.45-14.28l-.9-.44zM23 48.5c-9.4 0-17.57-5.57-21.55-13.72l-.9.44C4.7 43.68 13.18 49.5 23 49.5v-1z"></path></svg><img alt="TensorFlow" src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/1_iDQvKoz7gGHc6YXqvqWWZQ(1).png" class="n cd od oc" width="40" height="40"></div></a></div><div class="il ac n"><div class="ag"><div style="flex: 1 1 0%;"><span class="ao b ap aq ar as n bq am"><div class="di ag ah in" data-test-id="postByline"><span class="ao dd ee aq cj io eg eh ip ej bq"><a class="av aw ax ay az ba bb bc bd be iq bh bi bj bk" href="https://medium.com/@tensorflow?source=post_recirc---------2------------------">TensorFlow</a><span> in <a href="https://medium.com/tensorflow?source=post_recirc---------2------------------" class="av aw ax ay az ba bb bc bd be iq bh bi bj bk">TensorFlow</a></span></span></div></span></div></div><span class="ao b ap aq ar as n at au"><span class="ao dd ee aq cj io eg eh ip ej at"><div><a class="av aw ax ay az ba bb bc bd be iq bh bi bj bk" href="https://medium.com/tensorflow/tf-jam-shooting-hoops-with-machine-learning-7a96e1236c32?source=post_recirc---------2------------------">Jul 23, 2018</a> · 10 min read</div></span></span></div></div></div><div class="ag ah"><div class="ag ah"><div class="et n eu"><div class=""><button class="bc ev ew ex ey ez fa fd"><svg width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="fb n"><div class="fc"><h4 class="ao dd ee aq at">3.8K </h4></div></div></div><div class="oj il ob cx ok n"></div><div class="fd"><div><div class="bz"><button class="av aw ax ay az ba bb bc bd be bf bg bh bi bj bk"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div></section></section></section></div></div></div><script class="hidden-originally">window.PARSELY = window.PARSELY || {autotrack: false}</script></div></div><script class="hidden-originally">window.__BUILD_ID__ = "development"</script><script class="hidden-originally">window.__GRAPHQL_URI__ = "https://medium.com/_/graphql"</script><script class="hidden-originally">window.__PRELOADED_STATE__ = {"config":{"nodeEnv":"production","version":"master-20190717-192838-7bea1a2452","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","iTunesAppId":"828256236","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","lightStep":{"name":"lite-web","host":"collector-medium.lightstep.com","token":"ce5be895bef60919541332990ac9fef2","appVersion":"master-20190717-192838-7bea1a2452"},"algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6LdAokEUAAAAAC7seICd4vtC8chDb3jIXDQulyUJ","sentry":{"dsn":"https:\u002F\u002F589e367c28ca47b195ce200d1507d18b@sentry.io\u002F1423575","environment":"production"},"isAmp":false,"googleAnalyticsCode":"UA-24232453-2","signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"]},"debug":{"requestId":"55ad64e4-3e27-431b-bb3b-e34026d000df","originalSpanCarrier":{"ot-tracer-spanid":"4e836f2d56ce1c0f","ot-tracer-traceid":"26e323ee41327021","ot-tracer-sampled":"true"}},"session":{"user":{"id":"732323c9b7ea"},"xsrf":"Xky5H6D1F2Dz"},"stats":{"itemCount":0,"sending":false,"timeout":null,"backup":{}},"navigation":{"showBranchBanner":null,"hideGoogleOneTap":false,"currentLocation":"https:\u002F\u002Fmedium.com\u002Ftensorflow\u002Fan-introduction-to-biomedical-image-analysis-with-tensorflow-and-dltk-2c25304e7c13","host":"medium.com","hostname":"medium.com"},"client":{"isBot":false,"isDnt":false,"isEu":false,"isNativeMedium":false,"isCustomDomain":false},"multiVote":{"clapsPerPost":{}},"metadata":{"faviconImageId":null}}</script><script class="hidden-originally">window.__APOLLO_STATE__ = {"User:732323c9b7ea":{"id":"732323c9b7ea","username":"sagarcadet","name":"Sagar Kumar","imageId":"1*k0EpKE9wT5-E8uKkx8KIug.jpeg","mediumMemberAt":0,"hasPastMemberships":false,"isPartnerProgramEnrolled":false,"email":"sagarcadet@gmail.com","unverifiedEmail":"","__typename":"User"},"ROOT_QUERY":{"viewer":{"type":"id","generated":false,"id":"User:732323c9b7ea","typename":"User"},"variantFlags":[{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.0","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.1","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.2","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.3","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.4","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.5","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.6","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.7","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.8","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.9","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.10","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.11","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.12","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.13","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.14","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.15","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.16","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.17","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.18","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.19","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.20","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.21","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.22","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.23","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.24","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.25","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.26","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.27","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.28","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.29","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.30","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.31","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.32","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.33","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.34","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.35","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.36","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.37","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.38","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.39","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.40","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.41","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.42","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.43","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.44","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.45","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.46","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.47","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.48","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.49","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.50","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.51","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.52","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.53","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.54","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.55","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.56","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.57","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.58","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.59","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.60","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.61","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.62","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.63","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.64","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.65","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.66","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.67","typename":"VariantFlag"}],"meterPost({\"postId\":\"2c25304e7c13\",\"postMeteringOptions\":{}})":{"type":"id","generated":false,"id":"MeteringInfo:singleton","typename":"MeteringInfo"},"postResult({\"id\":\"2c25304e7c13\"})":{"type":"id","generated":false,"id":"Post:2c25304e7c13","typename":"Post"},"notificationsConnection({})":{"type":"id","generated":true,"id":"$ROOT_QUERY.notificationsConnection({})","typename":"NotificationsConnection"},"notificationStatus":{"type":"id","generated":true,"id":"$ROOT_QUERY.notificationStatus","typename":"NotificationStatus"}},"ROOT_QUERY.variantFlags.0":{"name":"allow_access","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.0.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.0.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.1":{"name":"allow_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.1.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.1.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.2":{"name":"allow_test_auth","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.2.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.2.valueType":{"__typename":"VariantFlagString","value":"disallow"},"ROOT_QUERY.variantFlags.3":{"name":"signin_services","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.3.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.3.valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap"},"ROOT_QUERY.variantFlags.4":{"name":"signup_services","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.4.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.4.valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap"},"ROOT_QUERY.variantFlags.5":{"name":"google_sign_in_android","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.5.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.5.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.6":{"name":"browsable_stream_config_bucket","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.6.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.6.valueType":{"__typename":"VariantFlagString","value":"curated-topics"},"ROOT_QUERY.variantFlags.7":{"name":"enable_dedicated_series_tab_api_ios","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.7.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.7.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.8":{"name":"enable_post_import","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.8.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.8.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.9":{"name":"available_monthly_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.9.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.9.valueType":{"__typename":"VariantFlagString","value":"60e220181034"},"ROOT_QUERY.variantFlags.10":{"name":"available_annual_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.10.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.10.valueType":{"__typename":"VariantFlagString","value":"2c754bcc2995"},"ROOT_QUERY.variantFlags.11":{"name":"disable_ios_resume_reading_toast","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.11.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.11.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.12":{"name":"is_not_medium_subscriber","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.12.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.12.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.13":{"name":"glyph_font_set","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.13.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.13.valueType":{"__typename":"VariantFlagString","value":"m2"},"ROOT_QUERY.variantFlags.14":{"name":"enable_branding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.14.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.14.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.15":{"name":"enable_branding_fonts","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.15.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.15.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.16":{"name":"enable_automated_mission_control_triggers","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.16.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.16.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.17":{"name":"enable_lite_profile","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.17.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.17.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.18":{"name":"enable_marketing_emails","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.18.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.18.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.19":{"name":"enable_topic_lifecycle_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.19.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.19.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.20":{"name":"enable_parsely","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.20.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.20.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.21":{"name":"enable_branch_io","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.21.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.21.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.22":{"name":"enable_ios_post_stats","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.22.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.22.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.23":{"name":"enable_lite_topics","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.23.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.23.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.24":{"name":"enable_lite_stories","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.24.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.24.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.25":{"name":"redis_read_write_splitting","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.25.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.25.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.26":{"name":"enable_tipalti_onboarding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.26.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.26.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.27":{"name":"enable_annual_renewal_reminder_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.27.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.27.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.28":{"name":"enable_janky_spam_rules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.28.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.28.valueType":{"__typename":"VariantFlagString","value":"users,posts"},"ROOT_QUERY.variantFlags.29":{"name":"enable_new_collaborative_filtering_data","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.29.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.29.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.30":{"name":"enable_google_one_tap","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.30.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.30.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.31":{"name":"enable_email_sign_in_captcha","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.31.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.31.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.32":{"name":"enable_primary_topic_for_mobile","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.32.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.32.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.33":{"name":"enable_lite_post","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.33.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.33.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.34":{"name":"enable_logged_out_homepage_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.34.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.34.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.35":{"name":"use_new_admin_topic_backend","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.35.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.35.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.36":{"name":"enable_quarantine_rules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.36.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.36.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.37":{"name":"enable_patronus_on_kubernetes","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.37.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.37.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.38":{"name":"pub_sidebar","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.38.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.38.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.39":{"name":"disable_mobile_featured_chunk","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.39.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.39.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.40":{"name":"enable_embedding_based_diversification","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.40.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.40.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.41":{"name":"enable_pub_newsletters","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.41.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.41.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.42":{"name":"enable_lite_pub_header_menu","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.42.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.42.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.43":{"name":"enable_lite_claps","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.43.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.43.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.44":{"name":"enable_lite_post_manager_gear_menu","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.44.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.44.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.45":{"name":"enable_live_user_post_scoring","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.45.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.45.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.46":{"name":"enable_lite_post_highlights","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.46.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.46.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.47":{"name":"enable_lite_post_highlights_view_only","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.47.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.47.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.48":{"name":"enable_tick_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.48.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.48.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.49":{"name":"enable_lite_post_cd","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.49.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.49.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.50":{"name":"enable_lite_private_notes","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.50.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.50.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.51":{"name":"enable_lite_private_notes_li_100","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.51.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.51.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.52":{"name":"enable_trumpland_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.52.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.52.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.53":{"name":"enable_lite_email_sign_in_flow","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.53.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.53.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.54":{"name":"enable_daily_read_digest_promo","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.54.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.54.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.55":{"name":"enable_lite_paywall_alert","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.55.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.55.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.56":{"name":"enable_edit_alt_text","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.56.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.56.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.57":{"name":"enable_serve_recs_from_ml_rank_homepage","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.57.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.57.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.58":{"name":"enable_serve_recs_from_ml_rank_digest","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.58.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.58.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.59":{"name":"enable_serve_recs_from_ml_rank_app_highlights","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.59.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.59.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.60":{"name":"enable_lite_thanks_to","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.60.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.60.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.61":{"name":"enable_lite_google_captcha","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.61.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.61.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.62":{"name":"enable_lite_branch_io","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.62.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.62.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.63":{"name":"enable_lite_notifications","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.63.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.63.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.64":{"name":"enable_lite_audio_upsells","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.64.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.64.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.65":{"name":"enable_ticks_digest_promo","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.65.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.65.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.66":{"name":"enable_lite_verify_email_butter_bar","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.66.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.66.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.67":{"name":"enable_lite_unread_notification_count","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.67.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.67.valueType":{"__typename":"VariantFlagBoolean","value":true},"MeteringInfo:singleton":{"__typename":"MeteringInfo","postIds":{"type":"json","json":[]},"maxUnlockCount":3,"unlocksRemaining":3},"Post:2c25304e7c13":{"__typename":"Post","creator":{"type":"id","generated":false,"id":"User:d836c760546d","typename":"User"},"isLocked":false,"lockedSource":"LOCKED_POST_SOURCE_NONE","id":"2c25304e7c13","collection":{"type":"id","generated":false,"id":"Collection:dca47aab201b","typename":"Collection"},"sequence":null,"firstPublishedAt":1530658623262,"isPublished":true,"title":"An Introduction to Biomedical Image Analysis with TensorFlow and DLTK","canonicalUrl":"","layerCake":0,"primaryTopic":null,"content({\"postMeteringOptions\":{}})":{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}})","typename":"PostContent"},"highlights":[{"type":"id","generated":false,"id":"Quote:anon_32e060993c2b","typename":"Quote"}],"latestPublishedVersion":"7cc3d902217e","mediumUrl":"https:\u002F\u002Fmedium.com\u002Ftensorflow\u002Fan-introduction-to-biomedical-image-analysis-with-tensorflow-and-dltk-2c25304e7c13","readingTime":11.467924528301886,"statusForCollection":"APPROVED","visibility":"PUBLIC","tags":[{"type":"id","generated":false,"id":"Tag:deep-learning","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:tensorflow","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:dltk","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:medical-imaging","typename":"Tag"}],"viewerClapCount":null,"readingList":"READING_LIST_NONE","clapCount":1285,"voterCount":242,"recommenders":[],"responsesCount":9,"collaborators":[{"type":"id","generated":false,"id":"Collaborator:2c25304e7c13-ed7237e90a6f","typename":"Collaborator"}],"inResponseToPostResult":null,"inResponseToMediaResource":null,"curationEligibleAt":0,"audioVersionUrl":null,"socialTitle":"","socialDek":"","metaDescription":"","latestPublishedAt":1530658623262,"previewContent":{"type":"id","generated":true,"id":"$Post:2c25304e7c13.previewContent","typename":"PreviewContent"},"previewImage":{"type":"id","generated":false,"id":"ImageMetadata:1*OjngIU6_yw_cIT-b8ConDA.png","typename":"ImageMetadata"},"updatedAt":1530658623786,"topics":[],"shareKey":null},"User:d836c760546d":{"id":"d836c760546d","__typename":"User","allowNotes":true,"name":"Martin Rajchl","isFollowing":false,"username":"m_rajchl","bio":"Imperial College Research Fellow; Computing & Brain Sci; Machine Learning, Computer Vision, Medical Imaging; http:\u002F\u002Fdltk.github.io; Own views; Vienna;","imageId":"0*Rqz0XPiSgFjDvJ7w.jpg","mediumMemberAt":0,"isBlocking":false,"isPartnerProgramEnrolled":false,"twitterScreenName":"m_rajchl"},"Collection:dca47aab201b":{"id":"dca47aab201b","__typename":"Collection","slug":"tensorflow","domain":null,"colorBehavior":"ACCENT_COLOR","name":"TensorFlow","logo":{"type":"id","generated":false,"id":"ImageMetadata:1*nhv3orDSO8ggJ6q6f-qu4Q.png","typename":"ImageMetadata"},"creator":{"type":"id","generated":false,"id":"User:b1d410cb9700","typename":"User"},"viewerCanManage":false,"avatar":{"type":"id","generated":false,"id":"ImageMetadata:1*iDQvKoz7gGHc6YXqvqWWZQ.png","typename":"ImageMetadata"},"isEnrolledInHightower":false,"navItems":[{"type":"id","generated":true,"id":"Collection:dca47aab201b.navItems.0","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:dca47aab201b.navItems.1","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:dca47aab201b.navItems.2","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:dca47aab201b.navItems.3","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:dca47aab201b.navItems.4","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:dca47aab201b.navItems.5","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:dca47aab201b.navItems.6","typename":"NavItem"}],"colorPalette":{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette","typename":"ColorPalette"},"viewerCanEditOwnPosts":false,"viewerCanEditPosts":false,"description":"TensorFlow is an end-to-end open source platform for machine learning.","viewerIsFollowing":false,"viewerIsSubscribedToLetters":false,"mediumNewsletterId":"","isUserSubscribedToMediumNewsletter":false,"ampEnabled":false,"twitterUsername":"tensorflow","facebookPageId":null,"favicon":{"type":"id","generated":false,"id":"ImageMetadata:","typename":"ImageMetadata"}},"ImageMetadata:1*nhv3orDSO8ggJ6q6f-qu4Q.png":{"id":"1*nhv3orDSO8ggJ6q6f-qu4Q.png","originalWidth":1068,"originalHeight":1077,"__typename":"ImageMetadata"},"User:b1d410cb9700":{"id":"b1d410cb9700","__typename":"User"},"ImageMetadata:1*iDQvKoz7gGHc6YXqvqWWZQ.png":{"id":"1*iDQvKoz7gGHc6YXqvqWWZQ.png","__typename":"ImageMetadata"},"Collection:dca47aab201b.navItems.0":{"title":"Announcements","url":"https:\u002F\u002Fmedium.com\u002Ftensorflow\u002Ftagged\u002Fannouncements","type":"TAG_NAV_ITEM","__typename":"NavItem"},"Collection:dca47aab201b.navItems.1":{"title":"Keras","url":"https:\u002F\u002Fmedium.com\u002Ftensorflow\u002Ftagged\u002Fkeras","type":"TAG_NAV_ITEM","__typename":"NavItem"},"Collection:dca47aab201b.navItems.2":{"title":"TensorFlow.js","url":"https:\u002F\u002Fmedium.com\u002Ftensorflow\u002Ftagged\u002Fjavascript","type":"TAG_NAV_ITEM","__typename":"NavItem"},"Collection:dca47aab201b.navItems.3":{"title":"Mobile","url":"https:\u002F\u002Fmedium.com\u002Ftensorflow\u002Ftagged\u002Fmobile","type":"TAG_NAV_ITEM","__typename":"NavItem"},"Collection:dca47aab201b.navItems.4":{"title":"Responsible AI","url":"https:\u002F\u002Fmedium.com\u002Ftensorflow\u002Ftagged\u002Fresponsible-ai","type":"TAG_NAV_ITEM","__typename":"NavItem"},"Collection:dca47aab201b.navItems.5":{"title":"All stories","url":"https:\u002F\u002Fmedium.com\u002Ftensorflow\u002Farchive","type":"ARCHIVE_NAV_ITEM","__typename":"NavItem"},"Collection:dca47aab201b.navItems.6":{"title":"tensorflow.org","url":"http:\u002F\u002Ftensorflow.org","type":"EXTERNAL_LINK_NAV_ITEM","__typename":"NavItem"},"$Collection:dca47aab201b.colorPalette.tintBackgroundSpectrum":{"backgroundColor":"#FFA5A7AB","colorPoints":[{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.tintBackgroundSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.tintBackgroundSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.tintBackgroundSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.tintBackgroundSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.tintBackgroundSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.tintBackgroundSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.tintBackgroundSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.tintBackgroundSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.tintBackgroundSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.tintBackgroundSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.tintBackgroundSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:dca47aab201b.colorPalette.tintBackgroundSpectrum.colorPoints.0":{"color":"#FFA5A7AB","point":0,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.tintBackgroundSpectrum.colorPoints.1":{"color":"#FFAFB0B4","point":0.1,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.tintBackgroundSpectrum.colorPoints.2":{"color":"#FFB8B9BD","point":0.2,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.tintBackgroundSpectrum.colorPoints.3":{"color":"#FFC2C2C5","point":0.3,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.tintBackgroundSpectrum.colorPoints.4":{"color":"#FFCBCBCE","point":0.4,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.tintBackgroundSpectrum.colorPoints.5":{"color":"#FFD4D4D6","point":0.5,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.tintBackgroundSpectrum.colorPoints.6":{"color":"#FFDDDDDF","point":0.6,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.tintBackgroundSpectrum.colorPoints.7":{"color":"#FFE6E5E7","point":0.7,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.tintBackgroundSpectrum.colorPoints.8":{"color":"#FFEFEEEF","point":0.8,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.tintBackgroundSpectrum.colorPoints.9":{"color":"#FFF8F6F7","point":0.9,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.tintBackgroundSpectrum.colorPoints.10":{"color":"#FFFFFEFF","point":1,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette":{"tintBackgroundSpectrum":{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.tintBackgroundSpectrum","typename":"ColorSpectrum"},"__typename":"ColorPalette","defaultBackgroundSpectrum":{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.defaultBackgroundSpectrum","typename":"ColorSpectrum"},"highlightSpectrum":{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.highlightSpectrum","typename":"ColorSpectrum"}},"$Collection:dca47aab201b.colorPalette.defaultBackgroundSpectrum":{"backgroundColor":"#FFFFFFFF","colorPoints":[{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.defaultBackgroundSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.defaultBackgroundSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.defaultBackgroundSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.defaultBackgroundSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.defaultBackgroundSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.defaultBackgroundSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.defaultBackgroundSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.defaultBackgroundSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.defaultBackgroundSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.defaultBackgroundSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.defaultBackgroundSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:dca47aab201b.colorPalette.defaultBackgroundSpectrum.colorPoints.0":{"color":"#FF828588","point":0,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.defaultBackgroundSpectrum.colorPoints.1":{"color":"#FF797B7F","point":0.1,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.defaultBackgroundSpectrum.colorPoints.2":{"color":"#FF707275","point":0.2,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.defaultBackgroundSpectrum.colorPoints.3":{"color":"#FF67686B","point":0.3,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.defaultBackgroundSpectrum.colorPoints.4":{"color":"#FF5D5E61","point":0.4,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.defaultBackgroundSpectrum.colorPoints.5":{"color":"#FF535456","point":0.5,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.defaultBackgroundSpectrum.colorPoints.6":{"color":"#FF494A4B","point":0.6,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.defaultBackgroundSpectrum.colorPoints.7":{"color":"#FF3E3F40","point":0.7,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.defaultBackgroundSpectrum.colorPoints.8":{"color":"#FF333334","point":0.8,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.defaultBackgroundSpectrum.colorPoints.9":{"color":"#FF272728","point":0.9,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.defaultBackgroundSpectrum.colorPoints.10":{"color":"#FF1A1A1A","point":1,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.highlightSpectrum":{"backgroundColor":"#FFFFFFFF","colorPoints":[{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.highlightSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.highlightSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.highlightSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.highlightSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.highlightSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.highlightSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.highlightSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.highlightSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.highlightSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.highlightSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:dca47aab201b.colorPalette.highlightSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:dca47aab201b.colorPalette.highlightSpectrum.colorPoints.0":{"color":"#FFF4F2F3","point":0,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.highlightSpectrum.colorPoints.1":{"color":"#FFF2F0F1","point":0.1,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.highlightSpectrum.colorPoints.2":{"color":"#FFF0EEEF","point":0.2,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.highlightSpectrum.colorPoints.3":{"color":"#FFEEECEE","point":0.3,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.highlightSpectrum.colorPoints.4":{"color":"#FFEBEBEC","point":0.4,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.highlightSpectrum.colorPoints.5":{"color":"#FFE9E9EB","point":0.5,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.highlightSpectrum.colorPoints.6":{"color":"#FFE7E7E9","point":0.6,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.highlightSpectrum.colorPoints.7":{"color":"#FFE5E5E7","point":0.7,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.highlightSpectrum.colorPoints.8":{"color":"#FFE3E3E6","point":0.8,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.highlightSpectrum.colorPoints.9":{"color":"#FFE1E1E4","point":0.9,"__typename":"ColorPoint"},"$Collection:dca47aab201b.colorPalette.highlightSpectrum.colorPoints.10":{"color":"#FFDFDFE3","point":1,"__typename":"ColorPoint"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}})":{"isLockedPreviewOnly":false,"__typename":"PostContent","bodyModel":{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel","typename":"RichText"}},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.sections.0":{"name":"6993","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.sections.1":{"name":"a14d","startIndex":97,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel":{"sections":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.sections.0","typename":"Section"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.sections.1","typename":"Section"}],"paragraphs":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.0","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.1","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.2","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.3","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.4","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.5","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.6","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.7","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.8","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.9","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.10","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.11","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.12","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.13","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.14","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.15","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.16","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.17","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.18","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.19","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.20","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.21","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.22","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.23","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.24","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.25","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.26","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.27","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.28","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.29","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.30","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.31","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.32","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.33","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.34","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.35","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.36","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.37","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.38","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.39","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.40","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.41","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.42","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.43","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.44","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.45","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.46","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.47","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.48","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.49","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.50","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.51","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.52","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.53","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.54","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.55","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.56","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.57","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.58","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.59","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.60","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.61","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.62","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.63","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.64","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.65","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.66","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.67","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.68","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.69","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.70","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.71","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.72","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.73","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.74","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.75","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.76","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.77","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.78","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.79","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.80","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.81","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.82","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.83","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.84","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.85","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.86","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.87","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.88","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.89","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.90","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.91","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.92","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.93","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.94","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.95","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.96","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.97","typename":"Paragraph"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.98","typename":"Paragraph"}],"__typename":"RichText"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.0":{"name":"6399","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"An Introduction to Biomedical Image Analysis with TensorFlow and DLTK","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.1":{"name":"64a0","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"By Martin Rajchl, S. Ira Ktena and Nick Pawlowski — Imperial College London","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.2":{"name":"1cc4","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"DLTK, the Deep Learning Toolkit for Medical Imaging extends TensorFlow to enable deep learning on biomedical images. It provides specialty ops and functions, implementations of models, tutorials (as used in this blog) and code examples for typical applications.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.2.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.2.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.2.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.2.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.2.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.2.markups.5","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.2.markups.0":{"type":"A","start":0,"end":4,"href":"https:\u002F\u002Fdltk.github.io\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.2.markups.1":{"type":"A","start":60,"end":70,"href":"https:\u002F\u002Fwww.tensorflow.org\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.2.markups.2":{"type":"A","start":185,"end":194,"href":"https:\u002F\u002Fgithub.com\u002FDLTK\u002FDLTK\u002Ftree\u002Fmaster\u002Fexamples\u002Ftutorials","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.2.markups.3":{"type":"A","start":222,"end":260,"href":"https:\u002F\u002Fgithub.com\u002FDLTK\u002FDLTK\u002Ftree\u002Fmaster\u002Fexamples\u002Fapplications","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.2.markups.4":{"type":"EM","start":10,"end":52,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.2.markups.5":{"type":"EM","start":70,"end":71,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.3":{"name":"1ad2","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"This blog post serves as a quick introduction to deep learning with biomedical images, where we will demonstrate a few issues and solutions to current engineering problems and show you how to get up and running with a prototype for your problem.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.4":{"name":"8a4c","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*pfe19FYqq9GKAUUgBBjXjg.jpeg","typename":"ImageMetadata"},"text":"website: https:\u002F\u002Fdltk.github.io; source: https:\u002F\u002Fgithub.com\u002FDLTK\u002FDLTK;","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.4.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.4.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*pfe19FYqq9GKAUUgBBjXjg.jpeg":{"id":"1*pfe19FYqq9GKAUUgBBjXjg.jpeg","originalHeight":512,"originalWidth":1024,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.4.markups.0":{"type":"A","start":9,"end":31,"href":"https:\u002F\u002Fdltk.github.io\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.4.markups.1":{"type":"A","start":41,"end":70,"href":"https:\u002F\u002Fgithub.com\u002FDLTK\u002FDLTK","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.5":{"name":"ed31","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"Overview","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.6":{"name":"89ad","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"What is biomedical image analysis and why is it needed? Biomedical images are measurements of the human body on different scales (i.e. microscopic, macroscopic, etc.). They come in a wide variety of imaging modalities (e.g. a CT scanner, an ultrasound machine, etc.) and measure a physical property of the human body (e.g. radiodensity, the opacity to X-rays). These images are interpreted by domain experts (e.g. a radiologist) for clinical tasks (e.g. a diagnosis) and have a large impact on decision making of physicians.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.6.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.6.markups.0":{"type":"STRONG","start":0,"end":55,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.7":{"name":"aaf4","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*1bNAG7ujkPbx64HWXKMoeg.png","typename":"ImageMetadata"},"text":"Examples of medical images (from top left to bottom right): Multi-sequence brain MRI: T1-weighted, T1 inversion recovery and T2 FLAIR channels; Stitched whole-body MRI; planar cardiac ultrasound; chest X-ray; cardiac cine MRI.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*1bNAG7ujkPbx64HWXKMoeg.png":{"id":"1*1bNAG7ujkPbx64HWXKMoeg.png","originalHeight":513,"originalWidth":956,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.8":{"name":"62bf","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Biomedical images are typically volumetric images (3D) and sometimes have an additional time dimension (4D) and\u002For multiple channels (4-5D) (e.g. multi-sequence MR images). The variation in biomedical images is quite different from that of a natural image (e.g. a photograph), as clinical protocols aim to stratify how an image is acquired (e.g. a patient is lying on his\u002Fher back, the head is not tilted, etc.). In their analysis, we aim to detect subtle differences (i.e. some small region indicating an abnormal finding).","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.9":{"name":"9de5","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Why computer vision and machine learning? Computer vision methods have long been employed to automatically analyze biomedical images. The recent advent of deep learning has replaced many other machine learning methods, because it avoids the creation of hand-engineering features, thus removing a critical source of error from the process. Additionally, the fast inference speeds of GPU-accelerated fully networks, allows us scale analyses to unprecedented amounts of data (e.g. 10⁶ subject images).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.9.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.9.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.9.markups.0":{"type":"A","start":478,"end":496,"href":"https:\u002F\u002Fimaging.ukbiobank.ac.uk\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.9.markups.1":{"type":"STRONG","start":0,"end":41,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.10":{"name":"e096","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Can we readily employ deep learning libraries for biomedical imaging?Why create DLTK? \nThe main reasons for creating DLTK were to include speciality tools for this domain out of the box. While many deep learning libraries expose low-level operations (e.g. tensor multiplications, etc.) to the developers, a lot of the higher-level specialty operations are missing for their use on volumetric images (e.g. differentiable 3D upsampling layers, etc.), and due to the additional spatial dimension(s) of the images, we can run into memory issues (e.g. storing a single copy of a database of 1k CT images, with image dimensions of 512x512x256 voxels in float32 is ~268 GB). Due to the different nature of acquisition, some images will require special pre-processing (e.g. intensity normalization, bias-field correction, de-noising, spatial normalization\u002Fregistration, etc).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.10.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.10.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.10.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.10.markups.3","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.10.markups.0":{"type":"A","start":117,"end":121,"href":"http:\u002F\u002Fdltk.github.io\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.10.markups.1":{"type":"STRONG","start":0,"end":87,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.10.markups.2":{"type":"EM","start":117,"end":122,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.10.markups.3":{"type":"EM","start":185,"end":187,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.11":{"name":"f226","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"File formats, headers & reading images","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.11.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.11.markups.0":{"type":"STRONG","start":0,"end":38,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.12":{"name":"0517","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"While many vendors of imaging modalities produce images in the DICOM standard format, saving volumes in series of 2D slices, many analysis libraries rely on formats more suited for computing and interfacing with medical images. We use the NifTI (or .nii format), originally developed for brain imaging, but widely used for most other volume images in both DLTK and for this tutorial. What this and other format saves is necessary information to reconstruct the image container and orient it in physical space.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.12.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.12.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.12.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.12.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.12.markups.4","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.12.markups.0":{"type":"A","start":63,"end":68,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FDICOM","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.12.markups.1":{"type":"A","start":239,"end":245,"href":"https:\u002F\u002Fnifti.nimh.nih.gov\u002Fnifti-1\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.12.markups.2":{"type":"EM","start":63,"end":68,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.12.markups.3":{"type":"EM","start":239,"end":245,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.12.markups.4":{"type":"EM","start":356,"end":361,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.13":{"name":"b694","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"For this, it requires specialty header information, and we will go through a few attributes to consider for deep learning:","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.14":{"name":"9bdf","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Dimensions and size store information about how to reconstruct the image (e.g. a volume into three dimensions with a size vector).","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.15":{"name":"82dc","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Data type","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.16":{"name":"a8d7","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Voxel spacing (also the physical dimensions of voxels, typically in mm)","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.17":{"name":"7aa6","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Physical coordinate system origin","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.18":{"name":"ae4d","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Direction","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.19":{"name":"394d","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Why are these attributes important? The network will train in the space of voxels, meaning we will create tensors of shape and dimensions [batch_size, dx, dy, dz, channels\u002Ffeatures] and feed it to the network. The network will train in that voxel space and assume that all images (also unseen test images) are normalised in that space or might have issues to generalise. In that voxel space, the feature extractors (e.g. convolutional layers) will assume that voxel dimensions are isotropic (i.e. are the same in each dimension) and all images are oriented the same way.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.19.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.19.markups.0":{"type":"STRONG","start":0,"end":35,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.20":{"name":"75c4","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"However, since most images are depicting physical space, we need to transform from that physical space into a common voxel space:","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.21":{"name":"92d9","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"If all images are oriented the same way (sometimes we require registration to spatially normalize images: check out MIRTK), we can compute the scaling transform from physical to voxel space via","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.21.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.21.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.21.markups.0":{"type":"A","start":116,"end":121,"href":"https:\u002F\u002Fbiomedia.doc.ic.ac.uk\u002Fsoftware\u002Fmirtk\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.21.markups.1":{"type":"EM","start":116,"end":121,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.22":{"name":"5628","__typename":"Paragraph","type":"PRE","href":null,"layout":null,"metadata":null,"text":"phys_coords = origin + voxel_spacing * voxel_coord","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.23":{"name":"6237","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"where all these information are vectors stored in the .nii header.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.24":{"name":"c51a","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Reading .nii images: There are several libraries to read .nii files and access the header information and parse it to obtain a reconstructed image container as a numpy array. We chose SimpleITK, a python wrapper around the ITK library, which allows us to import additional image filters for pre-processing and other tasks:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.24.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.24.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.24.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.24.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.24.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.24.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.24.markups.6","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.24.markups.0":{"type":"A","start":162,"end":167,"href":"http:\u002F\u002Fwww.numpy.org\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.24.markups.1":{"type":"A","start":184,"end":193,"href":"http:\u002F\u002Fwww.simpleitk.org\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.24.markups.2":{"type":"A","start":223,"end":226,"href":"https:\u002F\u002Fitk.org\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.24.markups.3":{"type":"STRONG","start":0,"end":20,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.24.markups.4":{"type":"EM","start":162,"end":167,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.24.markups.5":{"type":"EM","start":184,"end":193,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.24.markups.6":{"type":"EM","start":223,"end":226,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.25":{"name":"17d1","__typename":"Paragraph","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.25.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:936e646e1a7cb606254959e929ba899f":{"id":"936e646e1a7cb606254959e929ba899f","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"","__typename":"MediaResource"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.25.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:936e646e1a7cb606254959e929ba899f","typename":"MediaResource"},"__typename":"Iframe"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.26":{"name":"3ba3","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"Data I\u002FO considerations","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.26.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.26.markups.0":{"type":"STRONG","start":0,"end":23,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.27":{"name":"d955","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Depending on the size of the training database, there are several options to feed .nii image data into the network graph. Each of these methods has specific trade-offs in terms of speed and can be a bottleneck during training. We will go through and explain three options:","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.28":{"name":"08a2","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"In memory & feeding dictionaries: We can create a tf.placeholder to the network graph and feed it via feed_dict during training. We read all .nii files from disk , process them in python (c.f. load_data()) and store all training examples in memory, where we feed from:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.28.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.28.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.28.markups.2","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.28.markups.0":{"type":"STRONG","start":0,"end":34,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.28.markups.1":{"type":"EM","start":50,"end":64,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.28.markups.2":{"type":"EM","start":193,"end":204,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.29":{"name":"acb4","__typename":"Paragraph","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.29.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:77c3ad5f0293bd66f0cd0b6fe3700e24":{"id":"77c3ad5f0293bd66f0cd0b6fe3700e24","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"","__typename":"MediaResource"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.29.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:77c3ad5f0293bd66f0cd0b6fe3700e24","typename":"MediaResource"},"__typename":"Iframe"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.30":{"name":"903a","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"TLDR: this direct approach is typically the fastest and easiest to implement, as it avoids continuously reading the data from disk, however requires to keep the entire database of training examples (and validation examples) in memory, which is not feasible for larger databases or larger image files.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.31":{"name":"4332","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Using a TFRecords database: For most deep learning problems on image volumes, the database of training examples is too large to fit into memory. The TFRecords format allows to serialise training examples and store them on disk with quick write access (i.e. parallel data reads):","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.31.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.31.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.31.markups.2","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.31.markups.0":{"type":"STRONG","start":0,"end":27,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.31.markups.1":{"type":"EM","start":8,"end":18,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.31.markups.2":{"type":"EM","start":149,"end":159,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.32":{"name":"f4ac","__typename":"Paragraph","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.32.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:2831ccde9f887e33e92dfe26d35a94b7":{"id":"2831ccde9f887e33e92dfe26d35a94b7","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"","__typename":"MediaResource"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.32.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:2831ccde9f887e33e92dfe26d35a94b7","typename":"MediaResource"},"__typename":"Iframe"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.33":{"name":"28d5","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The format can directly interface with TensorFlow and can be directly integrated into a training loop in a tf.graph:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.33.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.33.markups.0":{"type":"EM","start":39,"end":50,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.34":{"name":"d69c","__typename":"Paragraph","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.34.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:cd7b8657e8fbc58d578f32c8aa648777":{"id":"cd7b8657e8fbc58d578f32c8aa648777","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"","__typename":"MediaResource"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.34.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:cd7b8657e8fbc58d578f32c8aa648777","typename":"MediaResource"},"__typename":"Iframe"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.35":{"name":"47ac","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"TLDR: TFRecords are fast means of accessing files from disk, but require to store yet another copy of the entire training database. If we are aiming to work with a database of several TB size, this could be prohibitive.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.35.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.35.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.35.markups.0":{"type":"EM","start":4,"end":5,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.35.markups.1":{"type":"EM","start":6,"end":16,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.36":{"name":"7b99","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Using native python generators: Lastly, we can use python generators, creating a read_fn() to directly load the image data…","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.36.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.36.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.36.markups.0":{"type":"STRONG","start":0,"end":31,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.36.markups.1":{"type":"EM","start":81,"end":90,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.37":{"name":"ee5a","__typename":"Paragraph","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.37.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:66d9755b4efc1fe6034cef4c133a0a93":{"id":"66d9755b4efc1fe6034cef4c133a0a93","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"","__typename":"MediaResource"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.37.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:66d9755b4efc1fe6034cef4c133a0a93","typename":"MediaResource"},"__typename":"Iframe"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.38":{"name":"5cd0","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"and tf.data.Dataset.from_generator() to queue the examples:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.38.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.38.markups.0":{"type":"EM","start":4,"end":36,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.39":{"name":"eb23","__typename":"Paragraph","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.39.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:0b91ef7382315937395647d97e30e8fa":{"id":"0b91ef7382315937395647d97e30e8fa","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"","__typename":"MediaResource"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.39.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:0b91ef7382315937395647d97e30e8fa","typename":"MediaResource"},"__typename":"Iframe"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.40":{"name":"9a58","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"TLDR: It avoids creating additional copies of the image database, however is considerably slower than TFRecords, due to the fact that the generator cannot parallel read and map functions.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.40.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.40.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.40.markups.0":{"type":"EM","start":4,"end":5,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.40.markups.1":{"type":"EM","start":102,"end":111,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.41":{"name":"3743","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Speed benchmarking & choosing a method: We ran these three methods of reading .nii files to TensorFlow and compared the time required to load and feed a fixed-size example database. All codes and results can be found in here.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.41.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.41.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.41.markups.0":{"type":"STRONG","start":0,"end":39,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.41.markups.1":{"type":"EM","start":92,"end":103,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.42":{"name":"8bf7","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The obviously fastest method was feeding from memory via placeholders in 5.6 seconds, followed by TFRecords with 31.1 seconds and the un-optimised reading from disk using python generators with 123.5 seconds. However, as long as the forward\u002Fbackward passes during training are the computational bottleneck, the speed of the data I\u002FO is negligible.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.42.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.42.markups.0":{"type":"EM","start":98,"end":108,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.43":{"name":"82cd","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"Data normalization","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.43.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.43.markups.0":{"type":"STRONG","start":0,"end":18,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.44":{"name":"0999","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"As with natural images, we can normalize biomedical image data, however the methods might slightly vary. The aim of normalization is to remove some variation in the data (e.g. different subject pose or differences in image contrast, etc.) that is known and so simplify the detection of subtle differences we are interested in instead (e.g. the presence of a pathology). Here, we will go over the most common forms of normalization:","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.45":{"name":"4eae","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Normalization of voxel intensities: This form is highly dependent on the imaging modality, the data was acquired with. Typical zero-mean, unit variance normalization is standard for qualitative images (e.g. weighted brain MR images, where the contrast is highly dependent on acquisition parameters, typically set by an expert). If we employ such statistical approaches, we use statistics from a full single volume, rather than an entire database.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.45.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.45.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.45.markups.0":{"type":"A","start":127,"end":165,"href":"https:\u002F\u002Fgithub.com\u002FDLTK\u002FDLTK\u002Fblob\u002Fdev\u002Fdltk\u002Fio\u002Fpreprocessing.py#L9","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.45.markups.1":{"type":"STRONG","start":0,"end":36,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.46":{"name":"3913","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"In contrast to this, quantitative imaging measures a physical quantity (e.g. radio-density in CT imaging, where the intensities are comparable across different scanners) and benefit from clipping and\u002For re-scaling, as simple range normalisation (e.g. to [-1,1]).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.46.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.46.markups.0":{"type":"A","start":218,"end":244,"href":"https:\u002F\u002Fgithub.com\u002FDLTK\u002FDLTK\u002Fblob\u002Fdev\u002Fdltk\u002Fio\u002Fpreprocessing.py#L39","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.47":{"name":"ee01","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*UmKR5B3wM8mdhsnVsz01hg.png","typename":"ImageMetadata"},"text":"Example intensity normalisation methods","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*UmKR5B3wM8mdhsnVsz01hg.png":{"id":"1*UmKR5B3wM8mdhsnVsz01hg.png","originalHeight":298,"originalWidth":1242,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.48":{"name":"4210","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Spatial normalisation: Normalising for image orientation avoids that the model will have to learn all possible orientations, which largely reduces the amount of training images required (see the importance of header attributes to know what orientation an image is in). We additionally account for voxel spacing, which may vary between images, even when acquired from the same scanner. This can be done by resampling to an isotropic resolution:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.48.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.48.markups.0":{"type":"STRONG","start":0,"end":23,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.49":{"name":"6b02","__typename":"Paragraph","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.49.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:f8dccba717dd779181028e832beaf27b":{"id":"f8dccba717dd779181028e832beaf27b","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"","__typename":"MediaResource"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.49.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:f8dccba717dd779181028e832beaf27b","typename":"MediaResource"},"__typename":"Iframe"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.50":{"name":"48ac","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"If further normalisation is required, we can use medical image registration packages (e.g. MIRTK, etc.) and register the images into the same space, so that voxel locations between images correspond to each other. A typical step in analysing structural brain MR images (e.g. T1-weighted MR images) is to register all images in the training database to a reference standard, such as a mean atlas (e.g. the MNI 305 atlas). Depending on the degrees of freedom of the registration method, this can also normalise for size (affine registration) or shape (deformable registration). These two variants are rather rarely used, as they remove some of the information in the image (i.e. shape information or size information), that might be important for analysis (e.g. a large heart might be predictive of heart disease).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.50.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.50.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.50.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.50.markups.3","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.50.markups.0":{"type":"A","start":91,"end":96,"href":"https:\u002F\u002Fbiomedia.doc.ic.ac.uk\u002Fsoftware\u002Fmirtk\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.50.markups.1":{"type":"A","start":405,"end":412,"href":"https:\u002F\u002Fwww.mcgill.ca\u002Fbic\u002Fsoftware\u002Ftools-data-analysis\u002Fanatomical-mri\u002Fatlases\u002Fmni-305","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.50.markups.2":{"type":"EM","start":91,"end":96,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.50.markups.3":{"type":"EM","start":405,"end":412,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.51":{"name":"ea7b","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"Data augmentation","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.52":{"name":"c0d3","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"More often than not, there is a limited amount of data available and some of the variation is not covered. A few examples include:","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.53":{"name":"dc38","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"soft-tissue organs, where a wide range of normal shapes exist","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.54":{"name":"d52f","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"pathologies, such as cancer lesions, which can largely vary in shape and location","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.55":{"name":"ab13","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"free-hand ultrasound images, where a lot of possible views are possible","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.56":{"name":"a9ab","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"In order to properly generalise to unseen test cases, we augment training images by simulating a variation in the data we aim to be robust against. Similarly to normalisation methods, we distinguish between intensity and spatial augmentations:","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.57":{"name":"48c2","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Examples of intensity augmentations:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.57.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.57.markups.0":{"type":"EM","start":0,"end":36,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.58":{"name":"2a2d","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Adding noise to training images generalise to noisy images","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.59":{"name":"ef22","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Adding a random offset or contrast to handle differences between images","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.60":{"name":"6dc6","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Examples of spatial augmentations:","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.61":{"name":"3fed","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Flipping the image tensor in directions on where to expect symmetry (e.g. a left\u002Fright flip on brain scans)","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.62":{"name":"027c","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Random deformations, (e.g. for mimicking differences in organ shape)","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.63":{"name":"ee18","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Rotations along axes (e.g. for simulating difference ultrasound view angles)","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.64":{"name":"808a","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Random cropping and training on patches","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.65":{"name":"4d2e","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*FgV1en0rLz5UFGQzdFdqvg.png","typename":"ImageMetadata"},"text":"Example intensity and spatial augmentation techniques","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*FgV1en0rLz5UFGQzdFdqvg.png":{"id":"1*FgV1en0rLz5UFGQzdFdqvg.png","originalHeight":797,"originalWidth":1251,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.66":{"name":"3558","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Important notes on augmentation and data I\u002FO: Depending on which augmentations are required or helpful, some operations are only available in python (e.g. random deformations), meaning that if a reading method is used that uses raw TensorFlow (i.e. TFRecords or tf.placeholder), they will need to be pre-computed and stored to disk, thus largely increasing the size of the training database.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.66.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.66.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.66.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.66.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.66.markups.4","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.66.markups.0":{"type":"A","start":155,"end":174,"href":"https:\u002F\u002Fgithub.com\u002FDLTK\u002FDLTK\u002Fblob\u002Fmaster\u002Fdltk\u002Fio\u002Faugmentation.py#L75","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.66.markups.1":{"type":"EM","start":0,"end":45,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.66.markups.2":{"type":"EM","start":232,"end":243,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.66.markups.3":{"type":"EM","start":249,"end":259,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.66.markups.4":{"type":"EM","start":262,"end":276,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.67":{"name":"4ea2","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"Class balancing","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.68":{"name":"3e88","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Domain expert interpretations (e.g. manual segmentations or disease classes) are a requirement during supervised learning from medical images. Typically, the image-level (e.g. a disease class) or voxel-level (i.e. segmentation) labels are not available in the same ratio, which means that the network will not see an equal amount of examples from each class during training. This does not have a large effect on accuracy if the class ratios are somewhat similar (e.g. 30\u002F70 for a binary classification case). However, since most losses are average costs on the entire batch, the network will first learn to correctly predict the most frequently seen class (e.g. background or normal cases, which are are typically more examples available of).","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.69":{"name":"57ca","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"A class imbalance during training will have a larger impact on rare phenomena (e.g. small lesions in image segmentation) and largely impact the test accuracy.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.70":{"name":"dc22","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"To avoid this drop, there are two typical approaches to combat class imbalances in datasets:","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.71":{"name":"6e12","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Class balancing via sampling: Here, we aim to correct the frequencies of seen examples during sampling. This can be done by a) sampling an equal amount from each class, b) under-sampling over-represented classes or c) over-sampling less frequent classes. In DLTK, we have an implementation for a), which can be found here. We sample random locations in the image volume and consider an extracted example, if it contains the class we are looking for.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.71.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.71.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.71.markups.0":{"type":"A","start":317,"end":321,"href":"https:\u002F\u002Fgithub.com\u002FDLTK\u002FDLTK\u002Fblob\u002Fblog\u002Fdltk\u002Fio\u002Faugmentation.py#L120","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.71.markups.1":{"type":"EM","start":258,"end":262,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.72":{"name":"6913","__typename":"Paragraph","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Class balancing via loss function: In contrast to typical voxel-wise mean losses (e.g. categorical cross-entropy, L2, etc.), we can a) use a loss function that is inherently balanced (e.g. smooth Dice loss, which is a mean Dice-coefficient across all classes) or b) re-weight the losses for each prediction by the class frequency (e.g. median-frequency re-weighted cross-entropy).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.72.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.72.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.72.markups.0":{"type":"A","start":189,"end":205,"href":"https:\u002F\u002Fgithub.com\u002FDLTK\u002FDLTK\u002Fblob\u002Fmaster\u002Fdltk\u002Fcore\u002Flosses.py#L51","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.72.markups.1":{"type":"A","start":266,"end":329,"href":"https:\u002F\u002Fgithub.com\u002FDLTK\u002FDLTK\u002Fblob\u002Fmaster\u002Fdltk\u002Fcore\u002Flosses.py#L10","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.73":{"name":"530c","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"Example application highlights","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.74":{"name":"f375","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"With all the basic knowledge provided in this blog post, we can now look into building full applications for deep learning on medical images with TensorFlow. We have implemented several typical applications using deep neural networks and will walk through a few of them to give you an insight on what problems you now can attempt to tackle.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.74.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.74.markups.0":{"type":"EM","start":146,"end":156,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.75":{"name":"be10","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Note: These example applications learn something meaningful, but were built for demo purposes, rather than high-performance implementations.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.75.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.75.markups.0":{"type":"EM","start":0,"end":140,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.76":{"name":"2f0f","__typename":"Paragraph","type":"H4","href":null,"layout":null,"metadata":null,"text":"Example datasets","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.76.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.76.markups.0":{"type":"STRONG","start":0,"end":16,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.77":{"name":"949c","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"We provide download and pre-processing scripts for all the examples below. For most cases (including the demos above), we used the IXI brain database. For image segmentation, we downloaded the MRBrainS13 challenge database, which you will need to register for, before you can download it.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.77.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.77.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.77.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.77.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.77.markups.4","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.77.markups.0":{"type":"A","start":11,"end":46,"href":"https:\u002F\u002Fgithub.com\u002FDLTK\u002FDLTK\u002Ftree\u002Fmaster\u002Fdata","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.77.markups.1":{"type":"A","start":131,"end":149,"href":"http:\u002F\u002Fbrain-development.org\u002Fixi-dataset\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.77.markups.2":{"type":"A","start":193,"end":222,"href":"http:\u002F\u002Fmrbrains13.isi.uu.nl\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.77.markups.3":{"type":"EM","start":131,"end":134,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.77.markups.4":{"type":"EM","start":193,"end":203,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.78":{"name":"332c","__typename":"Paragraph","type":"H4","href":null,"layout":null,"metadata":null,"text":"Image segmentation of multi-channel brain MR images","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.79":{"name":"f7cf","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*OjngIU6_yw_cIT-b8ConDA.png","typename":"ImageMetadata"},"text":"Tensorboard visualisation of multi-sequence image inputs, target labels and predictions","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*OjngIU6_yw_cIT-b8ConDA.png":{"id":"1*OjngIU6_yw_cIT-b8ConDA.png","originalHeight":864,"originalWidth":1113,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.80":{"name":"2954","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"This image segmentation application learns to predict brain tissues and white matter lesions from multi-sequence MR images (T1-weighted, T1 inversion recovery and T2 FLAIR) on the small (N=5) MRBrainS challenge dataset. It uses a 3D U-Net-like network with residual units as feature extractors and tracks the Dice coefficient accuracy for each label in TensorBoard.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.80.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.80.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.80.markups.0":{"type":"EM","start":192,"end":201,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.80.markups.1":{"type":"EM","start":353,"end":364,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.81":{"name":"8bb0","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The code and instructions can be found here.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.81.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.81.markups.0":{"type":"A","start":39,"end":43,"href":"https:\u002F\u002Fgithub.com\u002FDLTK\u002FDLTK\u002Ftree\u002Fmaster\u002Fexamples\u002Fapplications\u002FMRBrainS13_tissue_segmentation","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.82":{"name":"e98a","__typename":"Paragraph","type":"H4","href":null,"layout":null,"metadata":null,"text":"Age regression and sex classification from T1-weighted brain MR images","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.82.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.82.markups.0":{"type":"STRONG","start":0,"end":70,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.83":{"name":"f813","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*s81FfZex1tzjAoxv7mSSYQ.png","typename":"ImageMetadata"},"text":"Example input T1-weighted brain MR images for regression and classification","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*s81FfZex1tzjAoxv7mSSYQ.png":{"id":"1*s81FfZex1tzjAoxv7mSSYQ.png","originalHeight":653,"originalWidth":1117,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.84":{"name":"971e","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Two similar applications employing a scalable 3D ResNet architecture learn to predict the subject’s age (regression) or the subject’s sex (classification) from T1–weighted brain MR images from the IXI database. The main difference between this applications is the loss function: While we train the regression network to predict the age as a continuous variable with a L2-loss (the mean squared differences between the predicted age and the real age), we use a categorical cross-entropy loss to predict the class of the sex.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.84.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.84.markups.0":{"type":"EM","start":197,"end":201,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.85":{"name":"1404","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The code and instructions for these applications can be found here: classification, regression.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.85.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.85.markups.1","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.85.markups.0":{"type":"A","start":68,"end":82,"href":"https:\u002F\u002Fgithub.com\u002FDLTK\u002FDLTK\u002Ftree\u002Fmaster\u002Fexamples\u002Fapplications\u002FIXI_HH_sex_classification_resnet","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.85.markups.1":{"type":"A","start":84,"end":94,"href":"https:\u002F\u002Fgithub.com\u002FDLTK\u002FDLTK\u002Ftree\u002Fmaster\u002Fexamples\u002Fapplications\u002FIXI_HH_age_regression_resnet","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.86":{"name":"0a81","__typename":"Paragraph","type":"H4","href":null,"layout":null,"metadata":null,"text":"Representation learning on 3T multi-channel brain MR images","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.86.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.86.markups.0":{"type":"STRONG","start":0,"end":59,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.87":{"name":"4683","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*lahtOexzm9GhpsHFYFMnPw.png","typename":"ImageMetadata"},"text":"Test images and reconstructions using a deep convolutional auto-encoder network","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*lahtOexzm9GhpsHFYFMnPw.png":{"id":"1*lahtOexzm9GhpsHFYFMnPw.png","originalHeight":470,"originalWidth":707,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.88":{"name":"69f2","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Here we demo the use of a deep convolutional autoencoder architecture, a powerful tool for representation learning: The network takes a multi-sequence MR image as input and aims to reconstruct them. By doing so, it compresses the information of the entire training database in its latent variables. The trained weights can also be used for transfer learning or information compression. Note, that the reconstructed images are very smooth: This might be due to the fact that this application uses an L2-loss function or the network being to small to properly encode detailed information.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.89":{"name":"7df4","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The code and instructions can be found here.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.89.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.89.markups.0":{"type":"A","start":39,"end":43,"href":"https:\u002F\u002Fgithub.com\u002FDLTK\u002FDLTK\u002Ftree\u002Fmaster\u002Fexamples\u002Fapplications\u002FIXI_HH_representation_learning_cae","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.90":{"name":"a00e","__typename":"Paragraph","type":"H4","href":null,"layout":null,"metadata":null,"text":"Simple image super-resolution on T1w brain MR images","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.91":{"name":"14c1","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*K1YrnwlXdgY15zms5VS01A.png","typename":"ImageMetadata"},"text":"Image super-resolution: original target image; downsampled input image; linear upsampled image; predicted super-resolution;","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*K1YrnwlXdgY15zms5VS01A.png":{"id":"1*K1YrnwlXdgY15zms5VS01A.png","originalHeight":446,"originalWidth":1480,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.92":{"name":"3a84","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Single image super-resolution aims to learn how to upsample and reconstruct high-resolution images from low resolution inputs. This simple implementation creates a low-resolution version of an image and the super-res network learns to upsample the image to its original resolution (here the up-sampling factor is [4,4,4]). Additionally, we compute a linearly upsampled version to show the difference to the reconstructed image.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.93":{"name":"4632","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"The code and instructions can be found here.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.93.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.93.markups.0":{"type":"A","start":39,"end":43,"href":"https:\u002F\u002Fgithub.com\u002FDLTK\u002FDLTK\u002Ftree\u002Fmaster\u002Fexamples\u002Fapplications\u002FIXI_HH_superresolution","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.94":{"name":"33b7","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"Lastly…","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.94.markups.0","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.94.markups.0":{"type":"STRONG","start":0,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.95":{"name":"a201","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"We hope that this tutorial has helped you to ease into the topic of deep learning on biomedical images. If you found it helpful, we appreciate you sharing it and following DLTK on github. If you require help with a similar problem, come to our gitter.io chat and ask us. Maybe some day we can host your application in the DLTK model zoo. Thanks for reading!","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.95.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.95.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.95.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.95.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.95.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.95.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.95.markups.6","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.95.markups.0":{"type":"A","start":180,"end":186,"href":"https:\u002F\u002Fgithub.com\u002FDLTK\u002FDLTK","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.95.markups.1":{"type":"A","start":244,"end":258,"href":"https:\u002F\u002Fgitter.im\u002FDLTK\u002FDLTK","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.95.markups.2":{"type":"A","start":327,"end":336,"href":"https:\u002F\u002Fgithub.com\u002FDLTK\u002Fmodels","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.95.markups.3":{"type":"EM","start":172,"end":177,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.95.markups.4":{"type":"EM","start":180,"end":186,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.95.markups.5":{"type":"EM","start":244,"end":253,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.95.markups.6":{"type":"EM","start":322,"end":327,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.96":{"name":"eba6","__typename":"Paragraph","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*iIbVhflH6hX-DCcQlJzZjw.png","typename":"ImageMetadata"},"text":"https:\u002F\u002Ftwitter.com\u002Fdltk_; https:\u002F\u002Fdltk.github.io; https:\u002F\u002Fgitter.im\u002FDLTK\u002FDLTK;","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.96.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.96.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.96.markups.2","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*iIbVhflH6hX-DCcQlJzZjw.png":{"id":"1*iIbVhflH6hX-DCcQlJzZjw.png","originalHeight":288,"originalWidth":850,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.96.markups.0":{"type":"A","start":0,"end":26,"href":"https:\u002F\u002Ftwitter.com\u002Fdltk_","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.96.markups.1":{"type":"A","start":27,"end":50,"href":"https:\u002F\u002Fdltk.github.io\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.96.markups.2":{"type":"A","start":51,"end":79,"href":"https:\u002F\u002Fgitter.im\u002FDLTK\u002FDLTK","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.97":{"name":"edc8","__typename":"Paragraph","type":"H3","href":null,"layout":null,"metadata":null,"text":"Resources","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.98":{"name":"e80d","__typename":"Paragraph","type":"P","href":null,"layout":null,"metadata":null,"text":"Tutorial code, example applications, DLTK source","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.98.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.98.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.98.markups.2","typename":"Markup"}],"iframe":null,"mixtapeMetadata":null},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.98.markups.0":{"type":"A","start":0,"end":13,"href":"https:\u002F\u002Fgithub.com\u002FDLTK\u002FDLTK\u002Ftree\u002Fmaster\u002Fexamples\u002Ftutorials","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.98.markups.1":{"type":"A","start":15,"end":35,"href":"https:\u002F\u002Fgithub.com\u002FDLTK\u002FDLTK\u002Ftree\u002Fmaster\u002Fexamples\u002Fapplications","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"$Post:2c25304e7c13.content({\"postMeteringOptions\":{}}).bodyModel.paragraphs.98.markups.2":{"type":"A","start":37,"end":48,"href":"https:\u002F\u002Fgithub.com\u002FDLTK\u002FDLTK","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Quote:anon_32e060993c2b":{"id":"anon_32e060993c2b","paragraphs":[{"type":"id","generated":true,"id":"Quote:anon_32e060993c2b.paragraphs.0","typename":"Paragraph"}],"__typename":"Quote","userId":"anon","startOffset":42,"endOffset":132,"user":null},"Quote:anon_32e060993c2b.paragraphs.0":{"name":"9de5","__typename":"Paragraph"},"Tag:deep-learning":{"id":"deep-learning","displayTitle":"Deep Learning","__typename":"Tag"},"Tag:tensorflow":{"id":"tensorflow","displayTitle":"TensorFlow","__typename":"Tag"},"Tag:dltk":{"id":"dltk","displayTitle":"Dltk","__typename":"Tag"},"Tag:medical-imaging":{"id":"medical-imaging","displayTitle":"Medical Imaging","__typename":"Tag"},"Collaborator:2c25304e7c13-ed7237e90a6f":{"id":"2c25304e7c13-ed7237e90a6f","user":{"type":"id","generated":false,"id":"User:ed7237e90a6f","typename":"User"},"state":"visible","__typename":"Collaborator"},"User:ed7237e90a6f":{"id":"ed7237e90a6f","name":"Nick Pawlowski","__typename":"User"},"ImageMetadata:":{"id":"","__typename":"ImageMetadata"},"$Post:2c25304e7c13.previewContent":{"subtitle":"By Martin Rajchl, S. Ira Ktena and Nick Pawlowski — Imperial College London","__typename":"PreviewContent"},"$ROOT_QUERY.notificationsConnection({}).notifications.0":{"notificationType":"post_recommended","occurredAt":1561744760199,"isUnread":false,"post":{"type":"id","generated":false,"id":"Post:535f3381d302","typename":"Post"},"actor":{"type":"id","generated":false,"id":"User:b84c7f2f7915","typename":"User"},"quote":null,"rollupItems":[],"__typename":"Notification","responsePost":null,"collection":null,"noteReply":null,"note":null,"milestoneArg":0},"Post:535f3381d302":{"id":"535f3381d302","title":"My experience at PyConfHyd","visibility":"PUBLIC","__typename":"Post"},"User:b84c7f2f7915":{"username":"mkignito","id":"b84c7f2f7915","name":"Mritunjay Sourav","imageId":"0*8kGl2Xj9umajjKai.jpg","mediumMemberAt":0,"__typename":"User"},"$ROOT_QUERY.notificationsConnection({}).notifications.1":{"notificationType":"users_following_you","occurredAt":1537029188723,"isUnread":false,"post":null,"actor":{"type":"id","generated":false,"id":"User:1d655a3b6ca6","typename":"User"},"quote":null,"rollupItems":[],"__typename":"Notification","responsePost":null,"collection":null,"noteReply":null,"note":null,"milestoneArg":0},"User:1d655a3b6ca6":{"username":"pwaila","id":"1d655a3b6ca6","name":"pranav waila","imageId":"0*KbhGyrPQyecj6Pzi.jpg","mediumMemberAt":1561054937874,"__typename":"User"},"$ROOT_QUERY.notificationsConnection({}).notifications.2":{"notificationType":"post_recommended","occurredAt":1528782972931,"isUnread":false,"post":{"type":"id","generated":false,"id":"Post:535f3381d302","typename":"Post"},"actor":{"type":"id","generated":false,"id":"User:6b3d10b412be","typename":"User"},"quote":null,"rollupItems":[],"__typename":"Notification","responsePost":null,"collection":null,"noteReply":null,"note":null,"milestoneArg":0},"User:6b3d10b412be":{"username":"louiscklaw","id":"6b3d10b412be","name":"Louis Law","imageId":"0*VO5kAcFWkZ3q556Z.","mediumMemberAt":0,"__typename":"User"},"$ROOT_QUERY.notificationsConnection({}).notifications.3":{"notificationType":"users_following_you","occurredAt":1528782970972,"isUnread":false,"post":null,"actor":{"type":"id","generated":false,"id":"User:6b3d10b412be","typename":"User"},"quote":null,"rollupItems":[],"__typename":"Notification","responsePost":null,"collection":null,"noteReply":null,"note":null,"milestoneArg":0},"$ROOT_QUERY.notificationsConnection({}).notifications.4":{"notificationType":"users_following_you","occurredAt":1512320768174,"isUnread":false,"post":null,"actor":{"type":"id","generated":false,"id":"User:da02698386fb","typename":"User"},"quote":null,"rollupItems":[],"__typename":"Notification","responsePost":null,"collection":null,"noteReply":null,"note":null,"milestoneArg":0},"User:da02698386fb":{"username":"rpranjan11","id":"da02698386fb","name":"Rajnish Ranjan","imageId":"0*Rcjhx_BfkMC2tHsn.","mediumMemberAt":0,"__typename":"User"},"$ROOT_QUERY.notificationsConnection({}).notifications.5":{"notificationType":"post_recommended_rollup","occurredAt":1512320717364,"isUnread":false,"post":{"type":"id","generated":false,"id":"Post:67c419ee39de","typename":"Post"},"actor":{"type":"id","generated":false,"id":"User:da02698386fb","typename":"User"},"quote":null,"rollupItems":[{"type":"id","generated":true,"id":"$ROOT_QUERY.notificationsConnection({}).notifications.5.rollupItems.0","typename":"Notification"},{"type":"id","generated":true,"id":"$ROOT_QUERY.notificationsConnection({}).notifications.5.rollupItems.1","typename":"Notification"},{"type":"id","generated":true,"id":"$ROOT_QUERY.notificationsConnection({}).notifications.5.rollupItems.2","typename":"Notification"},{"type":"id","generated":true,"id":"$ROOT_QUERY.notificationsConnection({}).notifications.5.rollupItems.3","typename":"Notification"}],"__typename":"Notification","responsePost":null,"collection":null,"noteReply":null,"note":null,"milestoneArg":0},"Post:67c419ee39de":{"id":"67c419ee39de","title":"How much we will be dependent on IoT in future? -(The Home Automation)","visibility":"PUBLIC","__typename":"Post"},"$ROOT_QUERY.notificationsConnection({}).notifications.5.rollupItems.0":{"actor":{"type":"id","generated":false,"id":"User:da02698386fb","typename":"User"},"__typename":"Notification"},"User:7e44752daffb":{"id":"7e44752daffb","__typename":"User"},"$ROOT_QUERY.notificationsConnection({}).notifications.5.rollupItems.1":{"actor":{"type":"id","generated":false,"id":"User:7e44752daffb","typename":"User"},"__typename":"Notification"},"User:3fbfd38c3352":{"id":"3fbfd38c3352","__typename":"User"},"$ROOT_QUERY.notificationsConnection({}).notifications.5.rollupItems.2":{"actor":{"type":"id","generated":false,"id":"User:3fbfd38c3352","typename":"User"},"__typename":"Notification"},"User:e5c46c63c307":{"id":"e5c46c63c307","__typename":"User","username":"prashant81995","name":"Prashant rai","imageId":"1*dSjQkKHP6FKvWhWU94oo1w.jpeg","mediumMemberAt":0},"$ROOT_QUERY.notificationsConnection({}).notifications.5.rollupItems.3":{"actor":{"type":"id","generated":false,"id":"User:e5c46c63c307","typename":"User"},"__typename":"Notification"},"$ROOT_QUERY.notificationsConnection({}).notifications.6":{"notificationType":"users_following_you","occurredAt":1507924239255,"isUnread":false,"post":null,"actor":{"type":"id","generated":false,"id":"User:e5c46c63c307","typename":"User"},"quote":null,"rollupItems":[],"__typename":"Notification","responsePost":null,"collection":null,"noteReply":null,"note":null,"milestoneArg":0},"$ROOT_QUERY.notificationsConnection({}).notifications.7":{"notificationType":"post_recommended","occurredAt":1507924222164,"isUnread":false,"post":{"type":"id","generated":false,"id":"Post:535f3381d302","typename":"Post"},"actor":{"type":"id","generated":false,"id":"User:e5c46c63c307","typename":"User"},"quote":null,"rollupItems":[],"__typename":"Notification","responsePost":null,"collection":null,"noteReply":null,"note":null,"milestoneArg":0},"$ROOT_QUERY.notificationsConnection({})":{"notifications":[{"type":"id","generated":true,"id":"$ROOT_QUERY.notificationsConnection({}).notifications.0","typename":"Notification"},{"type":"id","generated":true,"id":"$ROOT_QUERY.notificationsConnection({}).notifications.1","typename":"Notification"},{"type":"id","generated":true,"id":"$ROOT_QUERY.notificationsConnection({}).notifications.2","typename":"Notification"},{"type":"id","generated":true,"id":"$ROOT_QUERY.notificationsConnection({}).notifications.3","typename":"Notification"},{"type":"id","generated":true,"id":"$ROOT_QUERY.notificationsConnection({}).notifications.4","typename":"Notification"},{"type":"id","generated":true,"id":"$ROOT_QUERY.notificationsConnection({}).notifications.5","typename":"Notification"},{"type":"id","generated":true,"id":"$ROOT_QUERY.notificationsConnection({}).notifications.6","typename":"Notification"},{"type":"id","generated":true,"id":"$ROOT_QUERY.notificationsConnection({}).notifications.7","typename":"Notification"}],"pagingInfo":{"type":"id","generated":true,"id":"$ROOT_QUERY.notificationsConnection({}).pagingInfo","typename":"Paging"},"__typename":"NotificationsConnection"},"$ROOT_QUERY.notificationsConnection({}).pagingInfo.next":{"limit":25,"page":null,"source":null,"to":"1507924222163","__typename":"PageParams"},"$ROOT_QUERY.notificationsConnection({}).pagingInfo":{"next":{"type":"id","generated":true,"id":"$ROOT_QUERY.notificationsConnection({}).pagingInfo.next","typename":"PageParams"},"__typename":"Paging"},"$ROOT_QUERY.notificationStatus":{"unreadNotificationCount":0,"__typename":"NotificationStatus"}}</script><script src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/manifest.a52df1ca.js" class="hidden-originally"></script><script src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/vendors_main.85c53847.chunk.js" class="hidden-originally"></script><script src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/main.a28ba6c2.chunk.js" class="hidden-originally"></script><script src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/vendors_screen.landingpages.trumpland_screen.post_screen.post.amp_screen.post.series_screen.profile__b319665e.f2be28a6.chunk.js" class="hidden-originally"></script>
<script src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/screen.post_screen.post.amp_screen.post.series_screen.profile_screen.sequence.library_screen.sequenc_036c6b37.7ef5bb14.chunk.js" class="hidden-originally"></script>
<script src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/screen.landingpages.trumpland_screen.post_screen.post.amp_screen.post.series_screen.profile_screen.s_5e114ebe.91098f27.chunk.js" class="hidden-originally"></script>
<script src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/screen.post_screen.post.amp_screen.sequence.post.07b8dcdc.chunk.js" class="hidden-originally"></script>
<script src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/screen.post.9a86c437.chunk.js" class="hidden-originally"></script><script class="hidden-originally">window.main();</script><script src="./An Introduction to Biomedical Image Analysis with TensorFlow and DLTK_files/p.js" async="" id="parsely-cf" class="hidden-originally"></script></body></html>